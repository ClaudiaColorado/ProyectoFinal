name: Dynamic Databricks Proyecto Final Deploy v7
on:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Install jq & curl
        run: sudo apt-get update && sudo apt-get install -y jq curl

      - name: Export multiple notebooks (raw)
        run: |
          ORIGIN_HOST=${{ secrets.DATABRICKS_ORIGIN_HOST }}
          ORIGIN_TOKEN=${{ secrets.DATABRICKS_ORIGIN_TOKEN }}
          NOTEBOOK_BASE="/Workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/1_Proceso"
          NOTEBOOKS=("drop_medallion" "00_prep_env" "01_ingest" "02_transform" "03_load" "04_dashboard_setup")
          mkdir -p notebooks_to_deploy
          for nb in "${NOTEBOOKS[@]}"; do
            echo "Exportando $nb en modo raw..."
            curl -s -X GET \
              -H "Authorization: Bearer $ORIGIN_TOKEN" \
              "$ORIGIN_HOST/api/2.0/workspace/export?path=$NOTEBOOK_BASE/$nb&format=SOURCE&direct_download=true" \
              --output "notebooks_to_deploy/$nb.py"
          done

      - name: Set destination base once
        run: |
          echo 'DEST_NOTEBOOK_BASE=/Workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/1_Proceso' >> $GITHUB_ENV

      - name: Deploy notebooks to Destination Workspace
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          DEST_BASE="${{ env.DEST_NOTEBOOK_BASE }}"
          for file in notebooks_to_deploy/*.py; do
            name=$(basename "$file" .py)
            dest_path="$DEST_BASE/$name"
            echo "Creando carpeta $DEST_BASE si no existe..."
            curl -s -X POST \
              -H "Authorization: Bearer $DEST_TOKEN" \
              -H "Content-Type: application/json" \
              -d "{\"path\":\"$DEST_BASE\"}" \
              "$DEST_HOST/api/2.0/workspace/mkdirs"
            echo "Importando $file → $dest_path"
            response=$(curl -s -X POST \
              -H "Authorization: Bearer $DEST_TOKEN" \
              -H "Content-Type: multipart/form-data" \
              -F "path=$dest_path" \
              -F "format=SOURCE" \
              -F "language=PYTHON" \
              -F "overwrite=true" \
              -F "content=@$file" \
              "$DEST_HOST/api/2.0/workspace/import")
            echo "Response: $response"
          done

      - name: Verify notebooks exist in destination
        run: |
          set -euo pipefail
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          DEST_BASE="${{ env.DEST_NOTEBOOK_BASE }}"
          echo "🔎 Verificando notebooks en $DEST_BASE ..."
          for nb in drop_medallion 00_prep_env 01_ingest 02_transform 03_load 04_dashboard_setup; do
            path="$DEST_BASE/$nb"
            echo "  → $path"
            resp=$(curl -s -H "Authorization: Bearer $DEST_TOKEN" \
                        "$DEST_HOST/api/2.0/workspace/get-status?path=$path")
            echo "$resp" | jq .
            type=$(echo "$resp" | jq -r '.object_type // empty')
            if [[ "$type" != "NOTEBOOK" ]]; then
              echo "❌ No se encontró como NOTEBOOK: $path"
              exit 1
            fi
          done
          echo "✅ Todos los notebooks existen en destino."

      - name: Check if workflow exists and delete if necessary
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          WORKFLOW_NAME="WF_ProyectoFinal"
          echo "Verificando si existe el workflow: $WORKFLOW_NAME"
          workflows_response=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.1/jobs/list")
          existing_job_id=$(echo "$workflows_response" | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name == $name) | .job_id')
          if [ "$existing_job_id" != "" ] && [ "$existing_job_id" != "null" ]; then
            echo "Workflow encontrado con ID: $existing_job_id. Eliminando..."
            delete_response=$(curl -s -X POST \
              -H "Authorization: Bearer $DEST_TOKEN" \
              -H "Content-Type: application/json" \
              -d "{\"job_id\": $existing_job_id}" \
              "$DEST_HOST/api/2.1/jobs/delete")
            echo "Delete response: $delete_response"
          else
            echo "No se encontró workflow existente con nombre: $WORKFLOW_NAME"
          fi

      - name: Get existing cluster ID
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          CLUSTER_NAME="cluster_SD"
          echo "Buscando cluster existente: $CLUSTER_NAME"
          clusters_response=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.0/clusters/list")
          echo "Clusters response: $clusters_response"
          cluster_id=$(echo "$clusters_response" | jq -r --arg name "$CLUSTER_NAME" '.clusters[]? | select(.cluster_name == $name) | .cluster_id')
          if [ "$cluster_id" != "" ] && [ "$cluster_id" != "null" ]; then
            echo "✅ Cluster encontrado: $CLUSTER_NAME con ID: $cluster_id"
            echo "CLUSTER_ID=$cluster_id" >> $GITHUB_ENV
          else
            echo "❌ No se encontró el cluster: $CLUSTER_NAME"
            echo "Clusters disponibles:"
            echo "$clusters_response" | jq -r '.clusters[]? | .cluster_name'
            exit 1
          fi

      - name: Recreate Databricks Workflow WF_ProyectoFinal (force clean)
        shell: bash
        env:
          DEST_HOST: ${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN: ${{ secrets.DATABRICKS_DEST_TOKEN }}
          DEST_BASE: ${{ env.DEST_NOTEBOOK_BASE }} 
          CLUSTER_ID: ${{ env.CLUSTER_ID }}
        run: |
          set -euo pipefail
          echo "Recreando workflow WF_ProyectoFinal con cluster ${CLUSTER_ID}"
          echo "DEST_BASE = ${DEST_BASE}"
          # 0) Si existe un job con ese nombre, eliminarlo (evita heredar config antigua tipo spark_python_task)
          existing_id=$(curl -sS -H "Authorization: Bearer ${DEST_TOKEN}" \
            "${DEST_HOST}/api/2.1/jobs/list" | jq -r \
            '.jobs[]? | select(.settings.name=="WF_ProyectoFinal") | .job_id' | head -n1)
          if [[ -n "${existing_id}" && "${existing_id}" != "null" ]]; then
            echo "Borrando job existente: ${existing_id}"
            curl -sS -X POST -H "Authorization: Bearer ${DEST_TOKEN}" \
              -H "Content-Type: application/json" \
              --data "{\"job_id\": ${existing_id}}" \
              "${DEST_HOST}/api/2.1/jobs/delete" >/dev/null
            echo "Job ${existing_id} eliminado"
          fi
          # 1) Construir settings.json sólo con notebook_task y rutas /Users/...
          jq -n \
            --arg base_users "$(echo "${DEST_BASE}" | sed 's#^/Workspace##')" \
            --arg cid  "$CLUSTER_ID" \
            '{
              name: "WF_ProyectoFinal",
              max_concurrent_runs: 1,
              tasks: [
                {
                  task_key: "drop_medallion",
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/drop_medallion") },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                },
                {
                  task_key: "00_prep_env",
                  depends_on: [ { task_key: "drop_medallion" } ],
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/00_prep_env") },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                },
                {
                  task_key: "01_ingest",
                  depends_on: [ { task_key: "00_prep_env" } ],
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/01_ingest") },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                },
                {
                  task_key: "02_transform",
                  depends_on: [ { task_key: "01_ingest" } ],
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/02_transform") },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                },
                {
                  task_key: "03_load",
                  depends_on: [ { task_key: "02_transform" } ],
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/03_load") },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                },
                {
                  task_key: "04_dashboard",
                  depends_on: [{ "task_key": "03_load" }],
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/04_dashboard_setup") },
                  base_parameters: {
                    catalog: "catalogo1002",
                    gold_schema: "gold",
                    create_views: "si"  
                  },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                }
              ]
            }' > settings.json
          echo "Settings a crear:"; jq . settings.json
          # 2) Crear el job limpio
          create_resp=$(curl -sS -X POST \
            -H "Authorization: Bearer ${DEST_TOKEN}" \
            -H "Content-Type: application/json" \
            --data-binary @settings.json \
            "${DEST_HOST}/api/2.1/jobs/create")
          echo "Create response:"; echo "${create_resp}" | jq .
          job_id=$(echo "${create_resp}" | jq -r '.job_id')
          if [[ -z "${job_id}" || "${job_id}" == "null" ]]; then
            echo "❌ Error al crear el job"; exit 1
          fi
          # 3) Verificación: deben ser notebook_task y apuntar a /Users/...
          echo "Verificando tareas del job ${job_id}..."
          curl -sS -H "Authorization: Bearer ${DEST_TOKEN}" \
            "${DEST_HOST}/api/2.1/jobs/get?job_id=${job_id}" \
            | jq '.settings.tasks[] | {
                  task_key,
                  source,
                  notebook_path: (if .notebook_task then .notebook_task.notebook_path else null end),
                  spark_python_task
              }'

      - name: Execute Workflow WF_ProyectoFinal
        env:
          DEST_HOST: ${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN: ${{ secrets.DATABRICKS_DEST_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          WORKFLOW_NAME="WF_ProyectoFinal"
          echo "🚀 Ejecutando workflow: $WORKFLOW_NAME"
          workflows_list=$(curl -sS \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            "$DEST_HOST/api/2.1/jobs/list")
          job_id=$(echo "$workflows_list" \
            | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name==$name) | .job_id' \
            | head -n1)
          if [[ -z "${job_id:-}" || "$job_id" == "null" ]]; then
            echo "❌ No se pudo encontrar el workflow '$WORKFLOW_NAME'"; exit 1
          fi
          echo "✅ Workflow $WORKFLOW_NAME encontrado con ID: $job_id"
          payload=$(jq -n --arg id "$job_id" '{job_id: ($id|tonumber)}')
          resp=$(curl -sS -X POST "$DEST_HOST/api/2.1/jobs/run-now" \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            -d "$payload" -w $'\n%{http_code}')
          code=$(echo "$resp" | tail -n1)
          body=$(echo "$resp" | sed '$d')
          if [[ ! "$code" =~ ^2 ]]; then
            echo "❌ Error al ejecutar el workflow (HTTP $code)"
            echo "Response: $body"
            exit 1
          fi
          run_id=$(echo "$body" | jq -r '.run_id')
          if [[ -z "${run_id:-}" || "$run_id" == "null" ]]; then
            echo "❌ No se obtuvo run_id"; echo "$body"; exit 1
          fi
          echo "🎯 Workflow ejecutado exitosamente!"
          echo "Run ID: $run_id"
          echo "WORKFLOW_RUN_ID=$run_id" >> $GITHUB_ENV
          echo "WORKFLOW_JOB_ID=$job_id" >> $GITHUB_ENV
          echo "🔗 URL del workflow: ${DEST_HOST}/#job/${job_id}/run/${run_id}"

      - name: Monitor Workflow Execution
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          RUN_ID="${{ env.WORKFLOW_RUN_ID }}"
          JOB_ID="${{ env.WORKFLOW_JOB_ID }}"
          echo "📊 Monitoreando ejecución del workflow WF_ProyectoFinal..."
          echo "Job ID: $JOB_ID"
          echo "Run ID: $RUN_ID"
          max_wait_time=600
          wait_time=0
          check_interval=30
          while [ $wait_time -lt $max_wait_time ]; do
            run_status=$(curl -s -X GET \
              -H "Authorization: Bearer $DEST_TOKEN" \
              "$DEST_HOST/api/2.1/jobs/runs/get?run_id=$RUN_ID")
            state=$(echo "$run_status" | jq -r '.state.life_cycle_state')
            result_state=$(echo "$run_status" | jq -r '.state.result_state // "RUNNING"')
            echo "⏱️  Estado actual: $state ($result_state) - Tiempo transcurrido: ${wait_time}s"
            echo "$run_status" | jq -r '.tasks[]? | "  📋 " + .task_key + ": " + .state.life_cycle_state + " (" + (.state.result_state // "RUNNING") + ")"'
            case "$state" in
              "TERMINATED")
                if [ "$result_state" = "SUCCESS" ]; then
                  echo "🎉 ¡Workflow WF_ProyectoFinal completado exitosamente!"
                  echo ""
                  echo "📈 Resumen de ejecución:"
                  echo "$run_status" | jq -r '.tasks[]? | "✅ " + .task_key + " → " + (.state.result_state // "SUCCESS")'
                  start_time=$(echo "$run_status" | jq -r '.start_time')
                  end_time=$(echo "$run_status" | jq -r '.end_time')
                  if [ "$start_time" != "null" ] && [ "$end_time" != "null" ]; then
                    duration=$((($end_time - $start_time) / 1000))
                    echo "⏰ Duración total: ${duration} segundos"
                  fi
                  exit 0
                else
                  echo "❌ Workflow terminó con errores: $result_state"
                  echo "📋 Detalles de las tareas:"
                  echo "$run_status" | jq -r '.tasks[]? | "❌ " + .task_key + ": " + (.state.result_state // "UNKNOWN")'
                  exit 1
                fi
                ;;
              "INTERNAL_ERROR"|"SKIPPED")
                echo "❌ Workflow falló con estado: $state"
                exit 1
                ;;
              *)
                echo "⏳ Workflow WF_ProyectoFinal aún ejecutándose..."
                ;;
            esac
            sleep $check_interval
            wait_time=$((wait_time + check_interval))
          done
          echo "⚠️  Timeout: El workflow aún se está ejecutando después de $max_wait_time segundos"
          echo "🔗 Verifica el estado en: $DEST_HOST/jobs/$JOB_ID/runs/$RUN_ID"
          echo "ℹ️  El workflow seguirá ejecutándose en Databricks"
          exit 0

      - name: Export Lakeview dashboard JSON (from ORIGIN)
        run: |
          set -euo pipefail
          ORIGIN_HOST=${{ secrets.DATABRICKS_ORIGIN_HOST }}
          ORIGIN_TOKEN=${{ secrets.DATABRICKS_ORIGIN_TOKEN }}

          # Ruta EXACTA en DEV (ORIGIN) donde se guarda el .lvdash.json
          WS_PATH="/Workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/2_Dashboard/Data_Jobs_Salaries_and_Trends.lvdash.json"

          # URL-encode del path para evitar "curl: (3) URL rejected"
          ENC_WS_PATH=$(jq -rn --arg p "$WS_PATH" '$p|@uri')

          echo "⬇️ Exportando ${WS_PATH}…"
          curl -sS -X GET \
           -H "Authorization: Bearer ${ORIGIN_TOKEN}" \
           "${ORIGIN_HOST}/api/2.0/workspace/export?path=${ENC_WS_PATH}&format=SOURCE&direct_download=true" \
           --output dash_from_ws.json

           test -s dash_from_ws.json || { echo "❌ Descarga vacía"; exit 1; }
          echo "✅ Export listo: dash_from_ws.json"


      - name: Upsert Lakeview dashboard in PROD (no publish yet)
        id: lakeview_upsert
        env:
          DEST_HOST: ${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN: ${{ secrets.DATABRICKS_DEST_TOKEN }}
          WH_ID: ${{ secrets.PROD_WAREHOUSE_ID }}
        run: |
          set -euo pipefail

          DISPLAY_NAME="Data Jobs - Salaries and Trends"
          PARENT_PATH="/Workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/2_Dashboards"
          DEST_PATH="${PARENT_PATH}/${DISPLAY_NAME}.lvdash.json"

          # Asegura carpeta destino (Workspace PROD)
          curl -sS -X POST \
            -H "Authorization: Bearer ${DEST_TOKEN}" \
            -H "Content-Type: application/json" \
            -d "{\"path\":\"${PARENT_PATH}\"}" \
            "${DEST_HOST}/api/2.0/workspace/mkdirs" >/dev/null || true

          # Carga el JSON exportado y compáctado como serialized_dashboard
          SD=$(jq -c . dash_from_ws.json)

          # Detecta si YA existe por ruta (¡codificada!)
          ENC_DEST_PATH=$(jq -rn --arg p "$DEST_PATH" '$p|@uri')
          st=$(curl -sS -H "Authorization: Bearer ${DEST_TOKEN}" \
            "${DEST_HOST}/api/2.0/workspace/get-status?path=${ENC_DEST_PATH}") || true
            echo "🔎 get-status:"; echo "$st" | jq . || true
            dash_id=$(echo "$st" | jq -r '.resource_id // empty')

          if [[ -n "$dash_id" ]]; then
            echo "🔁 PATCH dashboard ${dash_id}"
            payload=$(jq -n --arg dn "$DISPLAY_NAME" --arg wh "$WH_ID" --arg sd "$SD" \
              '{display_name:$dn, warehouse_id:$wh, serialized_dashboard:$sd}')
            resp=$(curl -sS -X PATCH \
              -H "Authorization: Bearer ${DEST_TOKEN}" \
              -H "Content-Type: application/json" \
              --data "$payload" \
              "${DEST_HOST}/api/2.0/lakeview/dashboards/${dash_id}")
          else
            echo "🆕 POST (create)"
            payload=$(jq -n --arg dn "$DISPLAY_NAME" --arg wh "$WH_ID" --arg sd "$SD" --arg pp "$PARENT_PATH" \
              '{display_name:$dn, warehouse_id:$wh, serialized_dashboard:$sd, parent_path:$pp}')
            resp=$(curl -sS -X POST \
              -H "Authorization: Bearer ${DEST_TOKEN}" \
              -H "Content-Type: application/json" \
              --data "$payload" \
              "${DEST_HOST}/api/2.0/lakeview/dashboards")
              dash_id=$(echo "$resp" | jq -r '.dashboard_id // .dashboardId // empty')

              # Si devolvió RESOURCE_ALREADY_EXISTS, pide el id por get-status otra vez
              if [[ -z "$dash_id" ]]; then
                st2=$(curl -sS -H "Authorization: Bearer ${DEST_TOKEN}" \
                "${DEST_HOST}/api/2.0/workspace/get-status?path=${ENC_DEST_PATH}") || true
                dash_id=$(echo "$st2" | jq -r '.resource_id // empty')
              fi
          fi

          echo "🧾 Respuesta:"; echo "$resp" | jq . || true
          test -n "$dash_id" || { echo "❌ No obtuve dashboard_id"; exit 1; }
          echo "dashboard_id=${dash_id}" >> $GITHUB_OUTPUT
          echo "✅ Upsert OK (dashboard_id=${dash_id})"

      - name: Publish Lakeview dashboard (AFTER pipeline)
        env:
          DEST_HOST: ${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN: ${{ secrets.DATABRICKS_DEST_TOKEN }}
        run: |
          set -euo pipefail

          DISPLAY_NAME="Data Jobs - Salaries and Trends"
          PARENT_PATH="/Workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/2_Dashboards"
          DEST_PATH="${PARENT_PATH}/${DISPLAY_NAME}.lvdash.json"

          dash_id="${{ steps.lakeview_upsert.outputs.dashboard_id }}"
          if [[ -z "$dash_id" ]]; then
            # Fallback por ruta (codificada)
            ENC_DEST_PATH=$(jq -rn --arg p "$DEST_PATH" '$p|@uri')
            st=$(curl -sS -H "Authorization: Bearer ${DEST_TOKEN}" \
                "${DEST_HOST}/api/2.0/workspace/get-status?path=${ENC_DEST_PATH}") || true
                dash_id=$(echo "$st" | jq -r '.resource_id // empty')
          fi

          test -n "$dash_id" || { echo "❌ No pude obtener dashboard_id para publicar"; exit 1; }

          echo  "🚀 Publicando ${dash_id}…"
          pub=$(curl -sS -X POST \
            -H "Authorization: Bearer ${DEST_TOKEN}" \
            -H "Content-Type: application/json" \
            --data '{"embed_credentials": true}' \
            "${DEST_HOST}/api/2.0/lakeview/dashboards/${dash_id}/published")
          echo "🧾 Publish response:"; echo "$pub" | jq .

          echo "🖼️ Abre en PROD:"
          echo "${DEST_HOST}/workspace${DEST_PATH}"

  
      - name: Clean up
        run: |
          rm -rf notebooks_to_deploy
          rm -f workflow_config.json

      - name: Done
        run: |
          echo "🎉 ¡Despliegue y ejecución completados exitosamente!"
          echo ""
          echo "📊 Resumen:"
          echo "✅ Notebooks desplegados: drop_medallion, 00_prep_env, 01_ingest, 02_transform, 03_load"
          echo "🚀 Workflow creado: WF_ProyectoFinal"
          echo "✅ Tareas configuradas:"
          echo "   - 🧹 Tarea1_notebook1 (drop_medallion)"
          echo "   - 🧰 Tarea2_notebook2 (00_prep_env)"
          echo "   - 🛢️⬇️ Tarea3_notebook2 (01_ingest)"
          echo "   - 🔄 Tarea4_notebook2 (02_transform)"
          echo "   - 🛢️⬆️Tarea5_notebook2 (03_load)"
          echo "   - ⚙️Tarea6_notebook2 (04_Dashboard Setup)"
          echo "   - 📊📈Tarea6_notebook2 (Dashboard)"
          echo "✅ Cluster existente: cluster_SD configurado"
          echo "🧭 Workflow ejecutado automáticamente"
          echo ""
          echo "🔗 Accede a tu workspace de Databricks para ver los resultados detallados"