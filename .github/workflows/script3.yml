name: Dynamic Databricks Proyecto Final Deploy v5
on:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Install jq & curl
        run: sudo apt-get update && sudo apt-get install -y jq curl

      - name: Export multiple notebooks (raw)
        run: |
          ORIGIN_HOST=${{ secrets.DATABRICKS_ORIGIN_HOST }}
          ORIGIN_TOKEN=${{ secrets.DATABRICKS_ORIGIN_TOKEN }}
          NOTEBOOK_BASE="/Workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/1_Proceso"
          NOTEBOOKS=("drop_medallion" "00_prep_env" "01_ingest" "02_transform" "03_load" "04_dashboard_setup")
          mkdir -p notebooks_to_deploy
          for nb in "${NOTEBOOKS[@]}"; do
            echo "Exportando $nb en modo raw..."
            curl -s -X GET \
              -H "Authorization: Bearer $ORIGIN_TOKEN" \
              "$ORIGIN_HOST/api/2.0/workspace/export?path=$NOTEBOOK_BASE/$nb&format=SOURCE&direct_download=true" \
              --output "notebooks_to_deploy/$nb.py"
          done

      - name: Set destination base once
        run: |
          echo 'DEST_NOTEBOOK_BASE=/Workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/1_Proceso' >> $GITHUB_ENV

      - name: Deploy notebooks to Destination Workspace
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          DEST_BASE="${{ env.DEST_NOTEBOOK_BASE }}"
          for file in notebooks_to_deploy/*.py; do
            name=$(basename "$file" .py)
            dest_path="$DEST_BASE/$name"
            echo "Creando carpeta $DEST_BASE si no existe..."
            curl -s -X POST \
              -H "Authorization: Bearer $DEST_TOKEN" \
              -H "Content-Type: application/json" \
              -d "{\"path\":\"$DEST_BASE\"}" \
              "$DEST_HOST/api/2.0/workspace/mkdirs"
            echo "Importando $file ‚Üí $dest_path"
            response=$(curl -s -X POST \
              -H "Authorization: Bearer $DEST_TOKEN" \
              -H "Content-Type: multipart/form-data" \
              -F "path=$dest_path" \
              -F "format=SOURCE" \
              -F "language=PYTHON" \
              -F "overwrite=true" \
              -F "content=@$file" \
              "$DEST_HOST/api/2.0/workspace/import")
            echo "Response: $response"
          done

      - name: Verify notebooks exist in destination
        run: |
          set -euo pipefail
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          DEST_BASE="${{ env.DEST_NOTEBOOK_BASE }}"
          echo "üîé Verificando notebooks en $DEST_BASE ..."
          for nb in drop_medallion 00_prep_env 01_ingest 02_transform 03_load 04_dashboard_setup; do
            path="$DEST_BASE/$nb"
            echo "  ‚Üí $path"
            resp=$(curl -s -H "Authorization: Bearer $DEST_TOKEN" \
                        "$DEST_HOST/api/2.0/workspace/get-status?path=$path")
            echo "$resp" | jq .
            type=$(echo "$resp" | jq -r '.object_type // empty')
            if [[ "$type" != "NOTEBOOK" ]]; then
              echo "‚ùå No se encontr√≥ como NOTEBOOK: $path"
              exit 1
            fi
          done
          echo "‚úÖ Todos los notebooks existen en destino."

      - name: Check if workflow exists and delete if necessary
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          WORKFLOW_NAME="WF_ProyectoFinal"
          echo "Verificando si existe el workflow: $WORKFLOW_NAME"
          workflows_response=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.1/jobs/list")
          existing_job_id=$(echo "$workflows_response" | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name == $name) | .job_id')
          if [ "$existing_job_id" != "" ] && [ "$existing_job_id" != "null" ]; then
            echo "Workflow encontrado con ID: $existing_job_id. Eliminando..."
            delete_response=$(curl -s -X POST \
              -H "Authorization: Bearer $DEST_TOKEN" \
              -H "Content-Type: application/json" \
              -d "{\"job_id\": $existing_job_id}" \
              "$DEST_HOST/api/2.1/jobs/delete")
            echo "Delete response: $delete_response"
          else
            echo "No se encontr√≥ workflow existente con nombre: $WORKFLOW_NAME"
          fi

       # === DASHBOARD: Exportar desde ORIGEN (Workspace) y subir a DESTINO (Workspace) ===
      - name: Export Lakeview dashboard JSON (from ORIGIN Workspace)
        run: |
          set -euo pipefail
          ORIGIN_HOST=${{ secrets.DATABRICKS_ORIGIN_HOST }}
          ORIGIN_TOKEN=${{ secrets.DATABRICKS_ORIGIN_TOKEN }}

          # Ruta del dashboard en ORIGEN (Workspace DEV)
          ORIGIN_DASH_PATH="/Workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/2_Dashboard"

          mkdir -p dashboards_to_deploy

          echo "üîé Validando que existe ${ORIGIN_DASH_PATH} en ORIGEN..."
          st=$(curl -sS -H "Authorization: Bearer ${ORIGIN_TOKEN}" \
              "${ORIGIN_HOST}/api/2.0/workspace/get-status?path=${ORIGIN_DASH_PATH}")
          echo "$st" | jq .

          # Acepta FILE o DASHBOARD 
          echo "$st" | jq -e 'select(.object_type=="FILE" or .object_type=="DASHBOARD" or .object_type=="NOTEBOOK")' >/dev/null \
            || { echo "‚ùå No se encontr√≥ el objeto en ${ORIGIN_DASH_PATH}"; exit 1; }

          echo "‚¨áÔ∏è Exportando JSON..."
          curl -sS -X GET \
            -H "Authorization: Bearer ${ORIGIN_TOKEN}" \
            "${ORIGIN_HOST}/api/2.0/workspace/export?path=${ORIGIN_DASH_PATH}&format=SOURCE&direct_download=true" \
            --output dashboards_to_deploy/Data_Jobs_Salaries_and_Trends.lvdash.json

            test -s dashboards_to_deploy/Data_Jobs_Salaries_and_Trends.lvdash.json \
             || { echo "‚ùå Export vac√≠o"; exit 1; }

            echo "‚úÖ Export listo ‚Üí dashboards_to_deploy/Data_Jobs_Salaries_and_Trends.lvdash.json"

      - name: Upload dashboard JSON to DEST Workspace
        run: |
          set -euo pipefail
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}

          SRC_FILE="dashboards_to_deploy/Data_Jobs_Salaries_and_Trends.lvdash.json"

          # Carpeta destino en PROD:
          DEST_DIR="/Workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/2_Dashboards"
          DEST_PATH="${DEST_DIR}/Data_Jobs_Salaries_and_Trends.lvdash.json"

          echo "üìÅ Creando carpeta destino si no existe..."
          curl -sS -X POST -H "Authorization: Bearer ${DEST_TOKEN}" \
            -H "Content-Type: application/json" \
            -d "{\"path\":\"${DEST_DIR}\"}" \
            "${DEST_HOST}/api/2.0/workspace/mkdirs" >/dev/null

          echo "‚¨ÜÔ∏è Subiendo JSON como FILE al Workspace de PROD..."
          
          resp=$(curl -sS -X POST \
            -H "Authorization: Bearer ${DEST_TOKEN}" \
            -H "Content-Type: multipart/form-data" \
            -F "path=${DEST_PATH}" \
            -F "format=SOURCE" \
            -F "overwrite=true" \
            -F "content=@${SRC_FILE}" \
            "${DEST_HOST}/api/2.0/workspace/import")

          echo "$resp" | jq . || true

          echo "üîé Verificando en destino..."
          st=$(curl -sS -H "Authorization: Bearer ${DEST_TOKEN}" \
            "${DEST_HOST}/api/2.0/workspace/get-status?path=${DEST_PATH}")
          echo "$st" | jq .

          obj_type=$(echo "$st" | jq -r '.object_type // empty')
          [[ "$obj_type" == "FILE" || "$obj_type" == "NOTEBOOK" || "$obj_type" == "DASHBOARD" ]] \
            || { echo "‚ùå No qued√≥ como objeto de Workspace: ${DEST_PATH}"; exit 1; }

          echo "‚úÖ JSON disponible en PROD: ${DEST_PATH}"

      - name: Get existing cluster ID
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          CLUSTER_NAME="cluster_SD"
          echo "Buscando cluster existente: $CLUSTER_NAME"
          clusters_response=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.0/clusters/list")
          echo "Clusters response: $clusters_response"
          cluster_id=$(echo "$clusters_response" | jq -r --arg name "$CLUSTER_NAME" '.clusters[]? | select(.cluster_name == $name) | .cluster_id')
          if [ "$cluster_id" != "" ] && [ "$cluster_id" != "null" ]; then
            echo "‚úÖ Cluster encontrado: $CLUSTER_NAME con ID: $cluster_id"
            echo "CLUSTER_ID=$cluster_id" >> $GITHUB_ENV
          else
            echo "‚ùå No se encontr√≥ el cluster: $CLUSTER_NAME"
            echo "Clusters disponibles:"
            echo "$clusters_response" | jq -r '.clusters[]? | .cluster_name'
            exit 1
          fi

      - name: Recreate Databricks Workflow WF_ProyectoFinal (force clean)
        shell: bash
        env:
          DEST_HOST: ${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN: ${{ secrets.DATABRICKS_DEST_TOKEN }}
          DEST_BASE: ${{ env.DEST_NOTEBOOK_BASE }} 
          CLUSTER_ID: ${{ env.CLUSTER_ID }}
        run: |
          set -euo pipefail
          echo "Recreando workflow WF_ProyectoFinal con cluster ${CLUSTER_ID}"
          echo "DEST_BASE = ${DEST_BASE}"
          # 0) Si existe un job con ese nombre, eliminarlo (evita heredar config antigua tipo spark_python_task)
          existing_id=$(curl -sS -H "Authorization: Bearer ${DEST_TOKEN}" \
            "${DEST_HOST}/api/2.1/jobs/list" | jq -r \
            '.jobs[]? | select(.settings.name=="WF_ProyectoFinal") | .job_id' | head -n1)
          if [[ -n "${existing_id}" && "${existing_id}" != "null" ]]; then
            echo "Borrando job existente: ${existing_id}"
            curl -sS -X POST -H "Authorization: Bearer ${DEST_TOKEN}" \
              -H "Content-Type: application/json" \
              --data "{\"job_id\": ${existing_id}}" \
              "${DEST_HOST}/api/2.1/jobs/delete" >/dev/null
            echo "Job ${existing_id} eliminado"
          fi
          # 1) Construir settings.json s√≥lo con notebook_task y rutas /Users/...
          jq -n \
            --arg base_users "$(echo "${DEST_BASE}" | sed 's#^/Workspace##')" \
            --arg cid  "$CLUSTER_ID" \
            '{
              name: "WF_ProyectoFinal",
              max_concurrent_runs: 1,
              tasks: [
                {
                  task_key: "drop_medallion",
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/drop_medallion") },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                },
                {
                  task_key: "00_prep_env",
                  depends_on: [ { task_key: "drop_medallion" } ],
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/00_prep_env") },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                },
                {
                  task_key: "01_ingest",
                  depends_on: [ { task_key: "00_prep_env" } ],
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/01_ingest") },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                },
                {
                  task_key: "02_transform",
                  depends_on: [ { task_key: "01_ingest" } ],
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/02_transform") },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                },
                {
                  task_key: "03_load",
                  depends_on: [ { task_key: "02_transform" } ],
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/03_load") },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                },
                {
                  task_key: "04_dashboard",
                  depends_on: [{ "task_key": "03_load" }],
                  source: "WORKSPACE",
                  notebook_task: { notebook_path: ($base_users + "/04_dashboard_setup") },
                  base_parameters: {
                    catalog: "catalogo1002",
                    gold_schema: "gold",
                    create_views: "si"  
                  },
                  existing_cluster_id: $cid,
                  timeout_seconds: 3600
                }
              ]
            }' > settings.json
          echo "Settings a crear:"; jq . settings.json
          # 2) Crear el job limpio
          create_resp=$(curl -sS -X POST \
            -H "Authorization: Bearer ${DEST_TOKEN}" \
            -H "Content-Type: application/json" \
            --data-binary @settings.json \
            "${DEST_HOST}/api/2.1/jobs/create")
          echo "Create response:"; echo "${create_resp}" | jq .
          job_id=$(echo "${create_resp}" | jq -r '.job_id')
          if [[ -z "${job_id}" || "${job_id}" == "null" ]]; then
            echo "‚ùå Error al crear el job"; exit 1
          fi
          # 3) Verificaci√≥n: deben ser notebook_task y apuntar a /Users/...
          echo "Verificando tareas del job ${job_id}..."
          curl -sS -H "Authorization: Bearer ${DEST_TOKEN}" \
            "${DEST_HOST}/api/2.1/jobs/get?job_id=${job_id}" \
            | jq '.settings.tasks[] | {
                  task_key,
                  source,
                  notebook_path: (if .notebook_task then .notebook_task.notebook_path else null end),
                  spark_python_task
              }'

      - name: Execute Workflow WF_ProyectoFinal
        env:
          DEST_HOST: ${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN: ${{ secrets.DATABRICKS_DEST_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          WORKFLOW_NAME="WF_ProyectoFinal"
          echo "üöÄ Ejecutando workflow: $WORKFLOW_NAME"
          workflows_list=$(curl -sS \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            "$DEST_HOST/api/2.1/jobs/list")
          job_id=$(echo "$workflows_list" \
            | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name==$name) | .job_id' \
            | head -n1)
          if [[ -z "${job_id:-}" || "$job_id" == "null" ]]; then
            echo "‚ùå No se pudo encontrar el workflow '$WORKFLOW_NAME'"; exit 1
          fi
          echo "‚úÖ Workflow $WORKFLOW_NAME encontrado con ID: $job_id"
          payload=$(jq -n --arg id "$job_id" '{job_id: ($id|tonumber)}')
          resp=$(curl -sS -X POST "$DEST_HOST/api/2.1/jobs/run-now" \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            -d "$payload" -w $'\n%{http_code}')
          code=$(echo "$resp" | tail -n1)
          body=$(echo "$resp" | sed '$d')
          if [[ ! "$code" =~ ^2 ]]; then
            echo "‚ùå Error al ejecutar el workflow (HTTP $code)"
            echo "Response: $body"
            exit 1
          fi
          run_id=$(echo "$body" | jq -r '.run_id')
          if [[ -z "${run_id:-}" || "$run_id" == "null" ]]; then
            echo "‚ùå No se obtuvo run_id"; echo "$body"; exit 1
          fi
          echo "üéØ Workflow ejecutado exitosamente!"
          echo "Run ID: $run_id"
          echo "WORKFLOW_RUN_ID=$run_id" >> $GITHUB_ENV
          echo "WORKFLOW_JOB_ID=$job_id" >> $GITHUB_ENV
          echo "üîó URL del workflow: ${DEST_HOST}/#job/${job_id}/run/${run_id}"

      - name: Monitor Workflow Execution
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          RUN_ID="${{ env.WORKFLOW_RUN_ID }}"
          JOB_ID="${{ env.WORKFLOW_JOB_ID }}"
          echo "üìä Monitoreando ejecuci√≥n del workflow WF_ProyectoFinal..."
          echo "Job ID: $JOB_ID"
          echo "Run ID: $RUN_ID"
          max_wait_time=600
          wait_time=0
          check_interval=30
          while [ $wait_time -lt $max_wait_time ]; do
            run_status=$(curl -s -X GET \
              -H "Authorization: Bearer $DEST_TOKEN" \
              "$DEST_HOST/api/2.1/jobs/runs/get?run_id=$RUN_ID")
            state=$(echo "$run_status" | jq -r '.state.life_cycle_state')
            result_state=$(echo "$run_status" | jq -r '.state.result_state // "RUNNING"')
            echo "‚è±Ô∏è  Estado actual: $state ($result_state) - Tiempo transcurrido: ${wait_time}s"
            echo "$run_status" | jq -r '.tasks[]? | "  üìã " + .task_key + ": " + .state.life_cycle_state + " (" + (.state.result_state // "RUNNING") + ")"'
            case "$state" in
              "TERMINATED")
                if [ "$result_state" = "SUCCESS" ]; then
                  echo "üéâ ¬°Workflow WF_ProyectoFinal completado exitosamente!"
                  echo ""
                  echo "üìà Resumen de ejecuci√≥n:"
                  echo "$run_status" | jq -r '.tasks[]? | "‚úÖ " + .task_key + " ‚Üí " + (.state.result_state // "SUCCESS")'
                  start_time=$(echo "$run_status" | jq -r '.start_time')
                  end_time=$(echo "$run_status" | jq -r '.end_time')
                  if [ "$start_time" != "null" ] && [ "$end_time" != "null" ]; then
                    duration=$((($end_time - $start_time) / 1000))
                    echo "‚è∞ Duraci√≥n total: ${duration} segundos"
                  fi
                  exit 0
                else
                  echo "‚ùå Workflow termin√≥ con errores: $result_state"
                  echo "üìã Detalles de las tareas:"
                  echo "$run_status" | jq -r '.tasks[]? | "‚ùå " + .task_key + ": " + (.state.result_state // "UNKNOWN")'
                  exit 1
                fi
                ;;
              "INTERNAL_ERROR"|"SKIPPED")
                echo "‚ùå Workflow fall√≥ con estado: $state"
                exit 1
                ;;
              *)
                echo "‚è≥ Workflow WF_ProyectoFinal a√∫n ejecut√°ndose..."
                ;;
            esac
            sleep $check_interval
            wait_time=$((wait_time + check_interval))
          done
          echo "‚ö†Ô∏è  Timeout: El workflow a√∫n se est√° ejecutando despu√©s de $max_wait_time segundos"
          echo "üîó Verifica el estado en: $DEST_HOST/jobs/$JOB_ID/runs/$RUN_ID"
          echo "‚ÑπÔ∏è  El workflow seguir√° ejecut√°ndose en Databricks"
          exit 0
      
      - name: Clean up
        run: |
          rm -rf notebooks_to_deploy
          rm -f workflow_config.json

      - name: Done
        run: |
          echo "üéâ ¬°Despliegue y ejecuci√≥n completados exitosamente!"
          echo ""
          echo "üìä Resumen:"
          echo "‚úÖ Notebooks desplegados: drop_medallion, 00_prep_env, 01_ingest, 02_transform, 03_load"
          echo "üöÄ Workflow creado: WF_ProyectoFinal"
          echo "‚úÖ Tareas configuradas:"
          echo "   - üßπ Tarea1_notebook1 (drop_medallion)"
          echo "   - üß∞ Tarea2_notebook2 (00_prep_env)"
          echo "   - üõ¢Ô∏è‚¨áÔ∏è Tarea3_notebook2 (01_ingest)"
          echo "   - üîÑ Tarea4_notebook2 (02_transform)"
          echo "   - üõ¢Ô∏è‚¨ÜÔ∏èTarea5_notebook2 (03_load)"
          echo "   - ‚öôÔ∏èTarea6_notebook2 (04_Dashboard Setup)"
          echo "   - üìäüìàTarea6_notebook2 (Dashboard)"
          echo "‚úÖ Cluster existente: cluster_SD configurado"
          echo "üß≠ Workflow ejecutado autom√°ticamente"
          echo ""
          echo "üîó Accede a tu workspace de Databricks para ver los resultados detallados"