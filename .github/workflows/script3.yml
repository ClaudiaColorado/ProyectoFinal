name: Deploy Databricks Workflow (WF_ProyectoFinal)

on:
  workflow_dispatch:
    inputs:
      run_now:
        description: "Ejecutar el workflow después del deploy"
        type: boolean
        default: true

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Asegurar jq
        shell: bash
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq

      - name: Setup Databricks CLI
        uses: databricks/setup-cli@v0

      # ⬇️ Respetar tus variables si ya vienen definidas; si no, tomar de secrets/vars
      - name: Cargar variables (conservar existentes)
        shell: bash
        run: |
          set -euo pipefail
          : "${DEST_HOST:=${{ secrets.DEST_HOST }}}"
          : "${DEST_TOKEN:=${{ secrets.DEST_TOKEN }}}"
          : "${DEST_CLUSTER_ID:=${{ vars.DEST_CLUSTER_ID }}}"
          : "${WORKSPACE_BASE:=${{ vars.WORKSPACE_BASE }}}"

          # Exponer host/base/cluster a siguientes steps (no exponemos el token)
          echo "DEST_HOST=$DEST_HOST" >> $GITHUB_ENV
          echo "DEST_CLUSTER_ID=$DEST_CLUSTER_ID" >> $GITHUB_ENV
          echo "WORKSPACE_BASE=$WORKSPACE_BASE" >> $GITHUB_ENV

          echo "🚩 DEST_HOST=$DEST_HOST"
          echo "🚩 DEST_CLUSTER_ID=$DEST_CLUSTER_ID"
          echo "🚩 WORKSPACE_BASE=$WORKSPACE_BASE"

      - name: Mostrar tareas del workflow
        shell: bash
        run: |
          echo "🚀 Workflow: Proyecto Final"
          echo "  1) 🧹  drop_medallion"
          echo "  2) 🧰  00_prep_env"
          echo "  3) 🛢️⬇️ 01_ingest"
          echo "  4) 🔄  02_transform"
          echo "  5) 🛢️⬆️ 03_load"

      - name: Render job.json
        shell: bash
        run: |
          set -euo pipefail

          cat > job.json <<'JSON'
          {
            "name": "WF_ProyectoFinal",
            "max_concurrent_runs": 1,
            "tasks": [
              {
                "task_key": "drop_medallion",
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/drop_medallion" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "00_prep_env",
                "depends_on": [{ "task_key": "drop_medallion" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/00_prep_env" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "01_ingest",
                "depends_on": [{ "task_key": "00_prep_env" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/01_ingest.py" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "02_transform",
                "depends_on": [{ "task_key": "01_ingest" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/02_transform.py" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "03_load",
                "depends_on": [{ "task_key": "02_transform" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/03_load" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              }
            ]
          }
          JSON

          sed -i "s#__WORKSPACE_BASE__#${WORKSPACE_BASE}#g" job.json
          sed -i "s/__CLUSTER_ID__/${DEST_CLUSTER_ID}/g" job.json

          jq . job.json

      - name: Upsert del job (create o reset)
        shell: bash
        env:
          DATABRICKS_HOST: ${{ env.DEST_HOST }}
          # Si no está en secrets, usa el que venga de tu entorno previo:
          DATABRICKS_TOKEN: ${{ secrets.DEST_TOKEN }}
        run: |
          set -euo pipefail
          # fallback al DEST_TOKEN exportado por ti si el secret no está seteado
          if [[ -z "${DATABRICKS_TOKEN:-}" ]]; then
            DATABRICKS_TOKEN="${DEST_TOKEN:-}"
          fi
          if [[ -z "${DATABRICKS_TOKEN}" ]]; then
            echo "❌ Falta DATABRICKS_TOKEN/DEST_TOKEN"; exit 1
          fi
          export DATABRICKS_TOKEN

          JOB_ID=$(databricks jobs list --output JSON | jq -r '.jobs[] | select(.settings.name=="WF_ProyectoFinal") | .job_id' || true)
          if [ -n "${JOB_ID}" ]; then
            echo "Job existente: ${JOB_ID} → reset con nueva configuración"
            databricks jobs reset --job-id "${JOB_ID}" --json-file job.json
          else
            echo "Creando nuevo job WF_ProyectoFinal"
            CREATE_OUT=$(databricks jobs create --json-file job.json)
            JOB_ID=$(echo "$CREATE_OUT" | jq -r '.job_id')
            if [ -z "${JOB_ID}" ] || [ "${JOB_ID}" = "null" ]; then
              echo "❌ No se obtuvo job_id al crear el job"; echo "$CREATE_OUT"; exit 1
            fi
          fi
          echo "JOB_ID=${JOB_ID}" >> $GITHUB_ENV
          echo "✅ Job listo con ID: ${JOB_ID}"

      # 🔎 Validación robusta (evita exit code 3 de jq)
      - name: Validar workflow y evitar exit code 3 de jq
        shell: bash
        env:
          HOST: ${{ env.DEST_HOST }}
          # mismo enfoque de fallback para el token:
          TOKEN_SECRET: ${{ secrets.DEST_TOKEN }}
        run: |
          set -Eeuo pipefail
          TOKEN="${TOKEN_SECRET:-${DEST_TOKEN:-}}"
          if [[ -z "${TOKEN}" ]]; then
            echo "❌ Falta token (DEST_TOKEN/secret)"; exit 1
          fi

          call_api () {
            local method="$1"
            local url="$2"
            local body="${3:-}"

            if [[ -n "$body" ]]; then
              resp=$(curl -sS -X "$method" "$url" \
                -H "Authorization: Bearer $TOKEN" \
                -H "Content-Type: application/json" \
                -d "$body" -w '\n%{http_code}')
            else
              resp=$(curl -sS -X "$method" "$url" \
                -H "Authorization: Bearer $TOKEN" \
                -H "Content-Type: application/json" \
                -w '\n%{http_code}')
            fi

            code=$(echo "$resp" | tail -n1)
            body=$(echo "$resp" | sed '$d')

            if [[ ! "$code" =~ ^2 ]]; then
              echo "❌ HTTP $code en $url"
              echo "—— Respuesta cruda ——"
              echo "$body"
              exit 1
            fi

            if ! echo "$body" | jq -e . >/dev/null 2>&1; then
              echo "❌ La respuesta no es JSON válido para $url"
              echo "—— Respuesta cruda ——"
              echo "$body"
              exit 1
            fi

            echo "$body"
          }

          echo "🔍 Validando la configuración del workflow creado..."
          if [[ -z "${JOB_ID:-}" ]]; then
            echo "❌ JOB_ID no está en el entorno"; exit 1
          fi

          WF=$(call_api GET "$HOST/api/2.1/jobs/get?job_id=${JOB_ID}")
          echo "✅ Workflow encontrado con ID: $(echo "$WF" | jq -r '.job_id')"
          echo "🧩 Nombre: $(echo "$WF" | jq -r '.settings.name')"
          echo "📦 Tareas:"
          echo "$WF" | jq -r '.settings.tasks[] | "- \(.task_key) -> \(.notebook_task.notebook_path)"'

      - name: Ejecutar workflow (opcional) y esperar a que termine
        if: ${{ inputs.run_now }}
        shell: bash
        env:
          DATABRICKS_HOST: ${{ env.DEST_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DEST_TOKEN }}
        run: |
          set -euo pipefail
          if [[ -z "${DATABRICKS_TOKEN:-}" ]]; then
            DATABRICKS_TOKEN="${DEST_TOKEN:-}"
          fi
          if [[ -z "${DATABRICKS_TOKEN}" ]]; then
            echo "❌ Falta DATABRICKS_TOKEN/DEST_TOKEN"; exit 1
          fi
          export DATABRICKS_TOKEN

          echo "⏩ Lanzando ejecución para JOB_ID=${JOB_ID}"
          RUN_OUT=$(databricks jobs run-now --job-id "${JOB_ID}")
          echo "$RUN_OUT" | jq .
          RUN_ID=$(echo "$RUN_OUT" | jq -r '.run_id')
          if [ -z "${RUN_ID}" ] || [ "${RUN_ID}" = "null" ]; then
            echo "❌ No se obtuvo run_id al ejecutar el job"; exit 1
          fi
          echo "RUN_ID=${RUN_ID}" >> $GITHUB_ENV

          echo "⏳ Esperando a que termine la ejecución…"
          databricks runs wait --run-id "${RUN_ID}" --timeout 7200
          echo "✅ Ejecución completada."
          echo "🧭 Workflow ejecutado automáticamente"
          echo ""
          echo "🔗 Accede a tu workspace de Databricks para ver los resultados detallados:"
          echo "   ${DATABRICKS_HOST}/#job/${JOB_ID}/run/${RUN_ID}"

      - name: Resumen con iconos
        if: ${{ always() }}
        shell: bash
        env:
          DATABRICKS_HOST: ${{ env.DEST_HOST }}
        run: |
          cat << EOF >> $GITHUB_STEP_SUMMARY
          ## 🧭 Pipeline — Proyecto Final
          1. 🧹 **drop_medallion**
          2. 🧰 **00_prep_env**
          3. 🛢️⬇️ **01_ingest**
          4. 🔄 **02_transform**
          5. 🛢️⬆️ **03_load**

          ---
          ### 🧭 Workflow ejecutado automáticamente
          🔗 Accede a tu workspace de Databricks para ver los resultados detallados:  
          ${DATABRICKS_HOST}/#job/${JOB_ID}/run/${RUN_ID}
          EOF
