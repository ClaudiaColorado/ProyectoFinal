name: Deploy Databricks Workflow (WF_ProyectoFinal)

on:
  workflow_dispatch:
    inputs:
      run_now:
        description: "Ejecutar el workflow despuÃ©s del deploy"
        type: boolean
        default: true

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Asegurar jq
        shell: bash
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq

      - name: Setup Databricks CLI
        uses: databricks/setup-cli@v0.270.0

      # ğŸ” Unificamos credenciales DEST y respetamos tus variables existentes
      - name: Exportar variables DEST al entorno
        shell: bash
        run: |
          set -euo pipefail
          # DEST: preferimos variables ya definidas; si no, caemos a secrets/vars
          HOST="${DEST_HOST:-${{ secrets.DEST_HOST }}}"
          [[ -z "$HOST" ]] && HOST="${{ secrets.DATABRICKS_DEST_HOST }}"
          TOKEN="${DEST_TOKEN:-${{ secrets.DEST_TOKEN }}}"
          [[ -z "$TOKEN" ]] && TOKEN="${{ secrets.DATABRICKS_DEST_TOKEN }}"
          CLUSTER="${DEST_CLUSTER_ID:-${{ vars.DEST_CLUSTER_ID }}}"
          BASE="${WORKSPACE_BASE:-${{ vars.WORKSPACE_BASE }}}"

          if [[ -z "$HOST" ]]; then echo "âŒ Falta DEST_HOST (o secret DEST_HOST / DATABRICKS_DEST_HOST)"; exit 1; fi
          if [[ -z "$TOKEN" ]]; then echo "âŒ Falta DEST_TOKEN (o secret DEST_TOKEN / DATABRICKS_DEST_TOKEN)"; exit 1; fi
          if [[ -z "$CLUSTER" ]]; then echo "âŒ Falta DEST_CLUSTER_ID (o var DEST_CLUSTER_ID)"; exit 1; fi
          if [[ -z "$BASE" ]]; then echo "âŒ Falta WORKSPACE_BASE (o var WORKSPACE_BASE)"; exit 1; fi

          echo "DEST_HOST=$HOST" >> $GITHUB_ENV
          echo "DEST_CLUSTER_ID=$CLUSTER" >> $GITHUB_ENV
          echo "WORKSPACE_BASE=$BASE" >> $GITHUB_ENV

          # El CLI usa estos dos nombres:
          echo "DATABRICKS_HOST=$HOST" >> $GITHUB_ENV
          echo "::add-mask::$TOKEN"
          echo "DATABRICKS_TOKEN=$TOKEN" >> $GITHUB_ENV

          echo "ğŸš© DEST_HOST=$HOST"
          echo "ğŸš© DEST_CLUSTER_ID=$CLUSTER"
          echo "ğŸš© WORKSPACE_BASE=$BASE"

      - name: Mostrar tareas del workflow
        shell: bash
        run: |
          echo "ğŸš€ Workflow: Proyecto Final"
          echo "  1) ğŸ§¹  drop_medallion"
          echo "  2) ğŸ§°  00_prep_env"
          echo "  3) ğŸ›¢ï¸â¬‡ï¸ 01_ingest"
          echo "  4) ğŸ”„  02_transform"
          echo "  5) ğŸ›¢ï¸â¬†ï¸ 03_load"

      # â¬‡ï¸ Export desde ORIGIN si hay secrets de origen (se omite si no existen)
      - name: Export multiple notebooks (raw) desde ORIGIN (opcional)
        shell: bash
        run: |
          set -euo pipefail
          ORIGIN_HOST='${{ secrets.DATABRICKS_ORIGIN_HOST }}'
          ORIGIN_TOKEN='${{ secrets.DATABRICKS_ORIGIN_TOKEN }}'
          # Usa ORIGIN_WORKSPACE_BASE si existe, sino el mismo WORKSPACE_BASE
          ORIGIN_BASE='${{ vars.ORIGIN_WORKSPACE_BASE }}'
          if [[ -z "$ORIGIN_BASE" || "$ORIGIN_BASE" == "null" ]]; then ORIGIN_BASE="${WORKSPACE_BASE}"; fi

          if [[ -z "$ORIGIN_HOST" || -z "$ORIGIN_TOKEN" ]]; then
            echo "â„¹ï¸ No hay credenciales de ORIGIN: se omite export."
          else
            echo "ğŸ“¤ Exportando desde ORIGIN: ${ORIGIN_BASE} â†’ ./exported"
            mkdir -p exported
            DATABRICKS_HOST="$ORIGIN_HOST" DATABRICKS_TOKEN="$ORIGIN_TOKEN" \
              databricks workspace export-dir "$ORIGIN_BASE" ./exported --overwrite
          fi

      # â¬‡ï¸ Import a DEST: si hay ./exported, lo sube; si no, continÃºa (asumes ya estÃ¡n en el workspace)
      - name: Deploy notebooks to Destination Workspace
        shell: bash
        run: |
          set -euo pipefail
          if [[ -d "./exported" ]]; then
            echo "ğŸ“¥ Importando a DEST: ./exported â†’ ${WORKSPACE_BASE}"
            databricks workspace import-dir ./exported "${WORKSPACE_BASE}" --overwrite
          else
            echo "â„¹ï¸ No existe ./exported; se asume que los notebooks ya estÃ¡n en ${WORKSPACE_BASE}."
          fi

      - name: Render job.json
        shell: bash
        run: |
          set -euo pipefail
          cat > job.json <<'JSON'
          {
            "name": "WF_ProyectoFinal",
            "max_concurrent_runs": 1,
            "tasks": [
              {
                "task_key": "drop_medallion",
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/drop_medallion" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "00_prep_env",
                "depends_on": [{ "task_key": "drop_medallion" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/00_prep_env" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "01_ingest",
                "depends_on": [{ "task_key": "00_prep_env" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/01_ingest.py" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "02_transform",
                "depends_on": [{ "task_key": "01_ingest" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/02_transform.py" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "03_load",
                "depends_on": [{ "task_key": "02_transform" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/03_load" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              }
            ]
          }
          JSON

          sed -i "s#__WORKSPACE_BASE__#${WORKSPACE_BASE}#g" job.json
          sed -i "s/__CLUSTER_ID__/${DEST_CLUSTER_ID}/g" job.json
          jq . job.json

      - name: Upsert del job (create o reset)
        shell: bash
        run: |
          set -euo pipefail
          JOB_ID=$(databricks jobs list --output JSON | jq -r '.jobs[] | select(.settings.name=="WF_ProyectoFinal") | .job_id' || true)
          if [ -n "${JOB_ID}" ]; then
            echo "Job existente: ${JOB_ID} â†’ reset con nueva configuraciÃ³n"
            databricks jobs reset --job-id "${JOB_ID}" --json-file job.json
          else
            echo "Creando nuevo job WF_ProyectoFinal"
            CREATE_OUT=$(databricks jobs create --json-file job.json)
            JOB_ID=$(echo "$CREATE_OUT" | jq -r '.job_id')
            if [ -z "${JOB_ID}" ] || [ "${JOB_ID}" = "null" ]; then
              echo "âŒ No se obtuvo job_id al crear el job"; echo "$CREATE_OUT"; exit 1
            fi
          fi
          echo "JOB_ID=${JOB_ID}" >> $GITHUB_ENV
          echo "âœ… Job listo con ID: ${JOB_ID}"

      # ğŸ” ValidaciÃ³n robusta: separa cÃ³digo HTTP y body, y valida JSON (anti exit code 3)
      - name: Validar workflow y evitar exit code 3 de jq
        shell: bash
        run: |
          set -Eeuo pipefail

          call_api () {
            local method="$1"
            local url="$2"
            local body="${3:-}"

            if [[ -n "$body" ]]; then
              resp=$(curl -sS -X "$method" "$url" \
                -H "Authorization: Bearer $DATABRICKS_TOKEN" \
                -H "Content-Type: application/json" \
                -d "$body" -w '\n%{http_code}')
            else
              resp=$(curl -sS -X "$method" "$url" \
                -H "Authorization: Bearer $DATABRICKS_TOKEN" \
                -H "Content-Type: application/json" \
                -w '\n%{http_code}')
            fi

            code=$(echo "$resp" | tail -n1)
            body=$(echo "$resp" | sed '$d')

            if [[ ! "$code" =~ ^2 ]]; then
              echo "âŒ HTTP $code en $url"
              echo "â€”â€” Respuesta cruda â€”â€”"
              echo "$body"
              exit 1
            fi

            if ! echo "$body" | jq -e . >/dev/null 2>&1; then
              echo "âŒ La respuesta no es JSON vÃ¡lido para $url"
              echo "â€”â€” Respuesta cruda â€”â€”"
              echo "$body"
              exit 1
            fi

            echo "$body"
          }

          echo "ğŸ” Validando la configuraciÃ³n del workflow creado..."
          WF=$(call_api GET "$DATABRICKS_HOST/api/2.1/jobs/get?job_id=${JOB_ID}")
          echo "âœ… Workflow encontrado con ID: $(echo "$WF" | jq -r '.job_id')"
          echo "ğŸ§© Nombre: $(echo "$WF" | jq -r '.settings.name')"
          echo "ğŸ“¦ Tareas:"
          echo "$WF" | jq -r '.settings.tasks[] | "- \(.task_key) -> \(.notebook_task.notebook_path)"'

      - name: Ejecutar workflow (opcional) y esperar a que termine
        if: ${{ inputs.run_now }}
        shell: bash
        run: |
          set -euo pipefail
          echo "â© Lanzando ejecuciÃ³n para JOB_ID=${JOB_ID}"
          RUN_OUT=$(databricks jobs run-now --job-id "${JOB_ID}")
          echo "$RUN_OUT" | jq .
          RUN_ID=$(echo "$RUN_OUT" | jq -r '.run_id')
          if [ -z "${RUN_ID}" ] || [ "${RUN_ID}" = "null" ]; then
            echo "âŒ No se obtuvo run_id al ejecutar el job"; exit 1
          fi
          echo "RUN_ID=${RUN_ID}" >> $GITHUB_ENV

          echo "â³ Esperando a que termine la ejecuciÃ³nâ€¦"
          databricks runs wait --run-id "${RUN_ID}" --timeout 7200
          echo "âœ… EjecuciÃ³n completada."
          echo "ğŸ§­ Workflow ejecutado automÃ¡ticamente"
          echo ""
          echo "ğŸ”— Accede a tu workspace de Databricks para ver los resultados detallados:"
          echo "   ${DATABRICKS_HOST}/#job/${JOB_ID}/run/${RUN_ID}"

      - name: Resumen con iconos
        if: ${{ always() }}
        shell: bash
        run: |
          cat << EOF >> $GITHUB_STEP_SUMMARY
          ## ğŸ§­ Pipeline â€” Proyecto Final
          1. ğŸ§¹ **drop_medallion**
          2. ğŸ§° **00_prep_env**
          3. ğŸ›¢ï¸â¬‡ï¸ **01_ingest**
          4. ğŸ”„ **02_transform**
          5. ğŸ›¢ï¸â¬†ï¸ **03_load**

          ---
          ### ğŸ§­ Workflow ejecutado automÃ¡ticamente
          ğŸ”— Accede a tu workspace de Databricks para ver los resultados detallados:  
          ${DATABRICKS_HOST}/#job/${JOB_ID}/run/${RUN_ID}
          EOF
