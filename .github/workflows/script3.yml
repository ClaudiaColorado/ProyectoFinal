name: Dynamic Databricks Proyecto Final Deploy v3
on:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Install jq & curl
        run: sudo apt-get update && sudo apt-get install -y jq curl

      - name: Export multiple notebooks (raw)
        run: |
          ORIGIN_HOST=${{ secrets.DATABRICKS_ORIGIN_HOST }}
          ORIGIN_TOKEN=${{ secrets.DATABRICKS_ORIGIN_TOKEN }}
          NOTEBOOK_BASE="/workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/1_Proceso"
          NOTEBOOKS=("drop_medallion" "00_prep_env" "01_ingest" "02_transform" "03_load")
          mkdir -p notebooks_to_deploy
          for nb in "${NOTEBOOKS[@]}"; do
            echo "Exportando $nb en modo raw..."
            curl -s -X GET \
              -H "Authorization: Bearer $ORIGIN_TOKEN" \
              "$ORIGIN_HOST/api/2.0/workspace/export?path=$NOTEBOOK_BASE/$nb&format=SOURCE&direct_download=true" \
              --output "notebooks_to_deploy/$nb.py"
          done

      - name: Set destination base once
        run: |
          echo 'DEST_NOTEBOOK_BASE=/Workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/1_Proceso' >> $GITHUB_ENV

      - name: Deploy notebooks to Destination Workspace
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          DEST_BASE="${{ env.DEST_NOTEBOOK_BASE }}"
          for file in notebooks_to_deploy/*.py; do
            name=$(basename "$file" .py)
            dest_path="$DEST_BASE/$name"
            echo "Creando carpeta $DEST_BASE si no existe..."
            curl -s -X POST \
              -H "Authorization: Bearer $DEST_TOKEN" \
              -H "Content-Type: application/json" \
              -d "{\"path\":\"$DEST_BASE\"}" \
              "$DEST_HOST/api/2.0/workspace/mkdirs"
            echo "Importando $file ‚Üí $dest_path"
            response=$(curl -s -X POST \
              -H "Authorization: Bearer $DEST_TOKEN" \
              -H "Content-Type: multipart/form-data" \
              -F "path=$dest_path" \
              -F "format=SOURCE" \
              -F "language=PYTHON" \
              -F "overwrite=true" \
              -F "content=@$file" \
              "$DEST_HOST/api/2.0/workspace/import")
            echo "Response: $response"
          done

      - name: Verify notebooks exist in destination
        run: |
          set -euo pipefail
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          DEST_BASE="${{ env.DEST_NOTEBOOK_BASE }}"
          echo "üîé Verificando notebooks en $DEST_BASE ..."
          for nb in drop_medallion 00_prep_env 01_ingest 02_transform 03_load; do
            path="$DEST_BASE/$nb"
            echo "  ‚Üí $path"
            resp=$(curl -s -H "Authorization: Bearer $DEST_TOKEN" \
                        "$DEST_HOST/api/2.0/workspace/get-status?path=$path")
            echo "$resp" | jq .
            type=$(echo "$resp" | jq -r '.object_type // empty')
            if [[ "$type" != "NOTEBOOK" ]]; then
              echo "‚ùå No se encontr√≥ como NOTEBOOK: $path"
              exit 1
            fi
          done
          echo "‚úÖ Todos los notebooks existen en destino."

      - name: Check if workflow exists and delete if necessary
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          WORKFLOW_NAME="WF_ADB"
          echo "Verificando si existe el workflow: $WORKFLOW_NAME"
          workflows_response=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.1/jobs/list")
          existing_job_id=$(echo "$workflows_response" | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name == $name) | .job_id')
          if [ "$existing_job_id" != "" ] && [ "$existing_job_id" != "null" ]; then
            echo "Workflow encontrado con ID: $existing_job_id. Eliminando..."
            delete_response=$(curl -s -X POST \
              -H "Authorization: Bearer $DEST_TOKEN" \
              -H "Content-Type: application/json" \
              -d "{\"job_id\": $existing_job_id}" \
              "$DEST_HOST/api/2.1/jobs/delete")
            echo "Delete response: $delete_response"
          else
            echo "No se encontr√≥ workflow existente con nombre: $WORKFLOW_NAME"
          fi

      - name: Get existing cluster ID
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          CLUSTER_NAME="cluster_SD"
          echo "Buscando cluster existente: $CLUSTER_NAME"
          clusters_response=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.0/clusters/list")
          echo "Clusters response: $clusters_response"
          cluster_id=$(echo "$clusters_response" | jq -r --arg name "$CLUSTER_NAME" '.clusters[]? | select(.cluster_name == $name) | .cluster_id')
          if [ "$cluster_id" != "" ] && [ "$cluster_id" != "null" ]; then
            echo "‚úÖ Cluster encontrado: $CLUSTER_NAME con ID: $cluster_id"
            echo "CLUSTER_ID=$cluster_id" >> $GITHUB_ENV
          else
            echo "‚ùå No se encontr√≥ el cluster: $CLUSTER_NAME"
            echo "Clusters disponibles:"
            echo "$clusters_response" | jq -r '.clusters[]? | .cluster_name'
            exit 1
          fi

      - name: Create Databricks Workflow WF_ProyectoFinal
        shell: bash
        env:
          DEST_HOST: ${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN: ${{ secrets.DATABRICKS_DEST_TOKEN }}
          DEST_BASE: ${{ env.DEST_NOTEBOOK_BASE }}
          CLUSTER_ID: ${{ env.CLUSTER_ID }}
        run: |
          set -euo pipefail
          echo "Creando workflow: WF_ProyectoFinal con cluster existente ID: $CLUSTER_ID"
          cat <<'JSON' > job.json
          {
            "name": "WF_ProyectoFinal",
            "max_concurrent_runs": 1,
            "tasks": [
              {
                "task_key": "drop_medallion",
                "source": "WORKSPACE",
                "notebook_task": {
                  "notebook_path": "__DEST_BASE__/drop_medallion"
                },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "00_prep_env",
                "depends_on": [{ "task_key": "drop_medallion" }],
                "source": "WORKSPACE",
                "notebook_task": {
                  "notebook_path": "__DEST_BASE__/00_prep_env"
                },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "01_ingest",
                "depends_on": [{ "task_key": "00_prep_env" }],
                "source": "WORKSPACE",
                "notebook_task": {
                  "notebook_path": "__DEST_BASE__/01_ingest"
                },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "02_transform",
                "depends_on": [{ "task_key": "01_ingest" }],
                "source": "WORKSPACE",
                "notebook_task": {
                  "notebook_path": "__DEST_BASE__/02_transform"
                },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "03_load",
                "depends_on": [{ "task_key": "02_transform" }],
                "source": "WORKSPACE",
                "notebook_task": {
                  "notebook_path": "__DEST_BASE__/03_load"
                },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              }
            ]
          }
          JSON
          sed -i "s#__DEST_BASE__#${DEST_BASE}#g" job.json
          sed -i "s/__CLUSTER_ID__/${CLUSTER_ID}/g" job.json
          echo "Job JSON final:"
          jq . job.json
          create_response=$(curl -s -X POST \
            -H "Authorization: Bearer ${DEST_TOKEN}" \
            -H "Content-Type: application/json" \
            --data-binary @job.json \
            "${DEST_HOST}/api/2.1/jobs/create")
          if echo "$create_response" | jq -e '.error_code=="RESOURCE_ALREADY_EXISTS"' >/dev/null 2>&1; then
            echo "Job ya existe; aplicando reset"
            job_id=$(curl -s -X GET -H "Authorization: Bearer ${DEST_TOKEN}" \
              "${DEST_HOST}/api/2.1/jobs/list" \
              | jq -r '.jobs[] | select(.settings.name=="WF_ProyectoFinal") | .job_id')
            curl -s -X POST \
              -H "Authorization: Bearer ${DEST_TOKEN}" \
              -H "Content-Type: application/json" \
              --data-binary @job.json \
              "${DEST_HOST}/api/2.1/jobs/reset?job_id=${job_id}" >/dev/null
            echo "Reset aplicado al job ${job_id}"
          else
            echo "Workflow creation response: ${create_response}"
            if ! echo "$create_response" | jq -e '.job_id' >/dev/null 2>&1; then
              echo "‚ùå Error al crear el workflow"
              echo "Response completo: ${create_response}"
              exit 1
            fi
            echo "Workflow creado correctamente"
          fi

      - name: Execute Workflow WF_ProyectoFinal
        env:
          DEST_HOST: ${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN: ${{ secrets.DATABRICKS_DEST_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          WORKFLOW_NAME="WF_ProyectoFinal"
          echo "üöÄ Ejecutando workflow: $WORKFLOW_NAME"
          workflows_list=$(curl -sS \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            "$DEST_HOST/api/2.1/jobs/list")
          job_id=$(echo "$workflows_list" \
            | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name==$name) | .job_id' \
            | head -n1)
          if [[ -z "${job_id:-}" || "$job_id" == "null" ]]; then
            echo "‚ùå No se pudo encontrar el workflow '$WORKFLOW_NAME'"; exit 1
          fi
          echo "‚úÖ Workflow $WORKFLOW_NAME encontrado con ID: $job_id"
          payload=$(jq -n --arg id "$job_id" '{job_id: ($id|tonumber)}')
          resp=$(curl -sS -X POST "$DEST_HOST/api/2.1/jobs/run-now" \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            -d "$payload" -w $'\n%{http_code}')
          code=$(echo "$resp" | tail -n1)
          body=$(echo "$resp" | sed '$d')
          if [[ ! "$code" =~ ^2 ]]; then
            echo "‚ùå Error al ejecutar el workflow (HTTP $code)"
            echo "Response: $body"
            exit 1
          fi
          run_id=$(echo "$body" | jq -r '.run_id')
          if [[ -z "${run_id:-}" || "$run_id" == "null" ]]; then
            echo "‚ùå No se obtuvo run_id"; echo "$body"; exit 1
          fi
          echo "üéØ Workflow ejecutado exitosamente!"
          echo "Run ID: $run_id"
          echo "WORKFLOW_RUN_ID=$run_id" >> $GITHUB_ENV
          echo "WORKFLOW_JOB_ID=$job_id" >> $GITHUB_ENV
          echo "üîó URL del workflow: ${DEST_HOST}/#job/${job_id}/run/${run_id}"

      - name: Monitor Workflow Execution
        run: |
          DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
          RUN_ID="${{ env.WORKFLOW_RUN_ID }}"
          JOB_ID="${{ env.WORKFLOW_JOB_ID }}"
          echo "üìä Monitoreando ejecuci√≥n del workflow WF_ProyectoFinal..."
          echo "Job ID: $JOB_ID"
          echo "Run ID: $RUN_ID"
          max_wait_time=600
          wait_time=0
          check_interval=30
          while [ $wait_time -lt $max_wait_time ]; do
            run_status=$(curl -s -X GET \
              -H "Authorization: Bearer $DEST_TOKEN" \
              "$DEST_HOST/api/2.1/jobs/runs/get?run_id=$RUN_ID")
            state=$(echo "$run_status" | jq -r '.state.life_cycle_state')
            result_state=$(echo "$run_status" | jq -r '.state.result_state // "RUNNING"')
            echo "‚è±Ô∏è  Estado actual: $state ($result_state) - Tiempo transcurrido: ${wait_time}s"
            echo "$run_status" | jq -r '.tasks[]? | "  üìã " + .task_key + ": " + .state.life_cycle_state + " (" + (.state.result_state // "RUNNING") + ")"'
            case "$state" in
              "TERMINATED")
                if [ "$result_state" = "SUCCESS" ]; then
                  echo "üéâ ¬°Workflow WF_ProyectoFinal completado exitosamente!"
                  echo ""
                  echo "üìà Resumen de ejecuci√≥n:"
                  echo "$run_status" | jq -r '.tasks[]? | "‚úÖ " + .task_key + " ‚Üí " + (.state.result_state // "SUCCESS")'
                  start_time=$(echo "$run_status" | jq -r '.start_time')
                  end_time=$(echo "$run_status" | jq -r '.end_time')
                  if [ "$start_time" != "null" ] && [ "$end_time" != "null" ]; then
                    duration=$((($end_time - $start_time) / 1000))
                    echo "‚è∞ Duraci√≥n total: ${duration} segundos"
                  fi
                  exit 0
                else
                  echo "‚ùå Workflow termin√≥ con errores: $result_state"
                  echo "üìã Detalles de las tareas:"
                  echo "$run_status" | jq -r '.tasks[]? | "‚ùå " + .task_key + ": " + (.state.result_state // "UNKNOWN")'
                  exit 1
                fi
                ;;
              "INTERNAL_ERROR"|"SKIPPED")
                echo "‚ùå Workflow fall√≥ con estado: $state"
                exit 1
                ;;
              *)
                echo "‚è≥ Workflow WF_ProyectoFinal a√∫n ejecut√°ndose..."
                ;;
            esac
            sleep $check_interval
            wait_time=$((wait_time + check_interval))
          done
          echo "‚ö†Ô∏è  Timeout: El workflow a√∫n se est√° ejecutando despu√©s de $max_wait_time segundos"
          echo "üîó Verifica el estado en: $DEST_HOST/jobs/$JOB_ID/runs/$RUN_ID"
          echo "‚ÑπÔ∏è  El workflow seguir√° ejecut√°ndose en Databricks"
          exit 0

      - name: Clean up
        run: |
          rm -rf notebooks_to_deploy
          rm -f workflow_config.json

      - name: Done
        run: |
          echo "üéâ ¬°Despliegue y ejecuci√≥n completados exitosamente!"
          echo ""
          echo "üìä Resumen:"
          echo "‚úÖ Notebooks desplegados: drop_medallion, 00_prep_env, 01_ingest, 02_transform, 03_load"
          echo "üöÄ Workflow creado: WF_ProyectoFinal"
          echo "‚úÖ Tareas configuradas:"
          echo "   - üßπ Tarea1_notebook1 (drop_medallion)"
          echo "   - üß∞ Tarea2_notebook2 (00_prep_env)"
          echo "   - üõ¢Ô∏è‚¨áÔ∏è Tarea3_notebook2 (01_ingest)"
          echo "   - üîÑ Tarea4_notebook2 (02_transform)"
          echo "   - üõ¢Ô∏è‚¨ÜÔ∏èTarea5_notebook2 (03_load)"
          echo "‚úÖ Cluster existente: cluster_SD configurado"
          echo "üß≠ Workflow ejecutado autom√°ticamente"
          echo ""
          echo "üîó Accede a tu workspace de Databricks para ver los resultados detallados"