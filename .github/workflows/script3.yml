name: Dynamic Databricks Proyecto Final Deploy v2
on:
  push:
    branches:
      - main
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v3
    
    - name: Install jq & curl
      run: sudo apt-get update && sudo apt-get install -y jq curl
    
    - name: Export multiple notebooks (raw)
      run: |
        ORIGIN_HOST=${{ secrets.DATABRICKS_ORIGIN_HOST }}
        ORIGIN_TOKEN=${{ secrets.DATABRICKS_ORIGIN_TOKEN }}
        NOTEBOOK_BASE="/Workspace/Users/claudiacoloradofc@gmail.com/ProyectoFinal/1_Proceso"
        NOTEBOOKS=("drop_medallion" "00_prep_env" "01_ingest" "02_transform" "03_load")  # Agrega mÃ¡s segÃºn necesitesas
        mkdir -p notebooks_to_deploy
        for nb in "${NOTEBOOKS[@]}"; do
          echo "Exportando $nb en modo raw..."
          curl -s -X GET \
            -H "Authorization: Bearer $ORIGIN_TOKEN" \
            "$ORIGIN_HOST/api/2.0/workspace/export?path=$NOTEBOOK_BASE/$nb&format=SOURCE&direct_download=true" \
            --output "notebooks_to_deploy/$nb.py"
        done
    
    - name: Deploy notebooks to Destination Workspace
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        DEST_BASE="/py/scripts/main"
        for file in notebooks_to_deploy/*.py; do
          name=$(basename "$file" .py)
          dest_path="$DEST_BASE/$name"
          echo "Creando carpeta $DEST_BASE si no existe..."
          curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"path\":\"$DEST_BASE\"}" \
            "$DEST_HOST/api/2.0/workspace/mkdirs"
          echo "Importando $file â†’ $dest_path"
          response=$(curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: multipart/form-data" \
            -F "path=$dest_path" \
            -F "format=SOURCE" \
            -F "language=PYTHON" \
            -F "overwrite=true" \
            -F "content=@$file" \
            "$DEST_HOST/api/2.0/workspace/import")
          echo "Response: $response"
        done
    
    - name: Check if workflow exists and delete if necessary
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        WORKFLOW_NAME="WF_ADB"
        
        echo "Verificando si existe el workflow: $WORKFLOW_NAME"
        
        # Listar todos los workflows y buscar por nombre
        workflows_response=$(curl -s -X GET \
          -H "Authorization: Bearer $DEST_TOKEN" \
          "$DEST_HOST/api/2.1/jobs/list")
        
        # Extraer job_id si existe el workflow
        existing_job_id=$(echo "$workflows_response" | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name == $name) | .job_id')
        
        if [ "$existing_job_id" != "" ] && [ "$existing_job_id" != "null" ]; then
          echo "Workflow encontrado con ID: $existing_job_id. Eliminando..."
          delete_response=$(curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"job_id\": $existing_job_id}" \
            "$DEST_HOST/api/2.1/jobs/delete")
          echo "Delete response: $delete_response"
        else
          echo "No se encontrÃ³ workflow existente con nombre: $WORKFLOW_NAME"
        fi
    
    - name: Get existing cluster ID
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        CLUSTER_NAME="cluster_SD"
        
        echo "Buscando cluster existente: $CLUSTER_NAME"
        
        # Obtener lista de clusters
        clusters_response=$(curl -s -X GET \
          -H "Authorization: Bearer $DEST_TOKEN" \
          "$DEST_HOST/api/2.0/clusters/list")
        
        echo "Clusters response: $clusters_response"
        
        # Extraer cluster_id del cluster especificado
        cluster_id=$(echo "$clusters_response" | jq -r --arg name "$CLUSTER_NAME" '.clusters[]? | select(.cluster_name == $name) | .cluster_id')
        
        if [ "$cluster_id" != "" ] && [ "$cluster_id" != "null" ]; then
          echo "âœ… Cluster encontrado: $CLUSTER_NAME con ID: $cluster_id"
          echo "CLUSTER_ID=$cluster_id" >> $GITHUB_ENV
        else
          echo "âŒ No se encontrÃ³ el cluster: $CLUSTER_NAME"
          echo "Clusters disponibles:"
          echo "$clusters_response" | jq -r '.clusters[]? | .cluster_name'
          exit 1
        fi
    
    - name: Create Databricks Workflow WF_ProyectoFinal
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        DEST_BASE="/py/scripts/main"
        CLUSTER_ID="${{ env.CLUSTER_ID }}"

        echo "Creando workflow: WF_ProyectoFinal con cluster existente ID: $CLUSTER_ID"

        # 1) Construir JSON vÃ¡lido (sin comas finales ni '...' ni comillas rotas)
        #    Usamos placeholders y luego los sustituimos con sed
        cat > job.json <<'JSON'
        {
          "name": "WF_ProyectoFinal",
          "max_concurrent_runs": 1,
          "tasks": [
            {
              "task_key": "drop_medallion",
              "notebook_task": {
                "notebook_path": "__DEST_BASE__/drop_medallion"
              },
              "existing_cluster_id": "__CLUSTER_ID__",
              "timeout_seconds": 3600
            },
            {
              "task_key": "00_prep_env",
              "depends_on": [{ "task_key": "drop_medallion" }],
              "notebook_task": {
                "notebook_path": "__DEST_BASE__/00_prep_env"
              },
              "existing_cluster_id": "__CLUSTER_ID__",
              "timeout_seconds": 3600
            },
            {
              "task_key": "01_ingest",
              "depends_on": [{ "task_key": "00_prep_env" }],
              "notebook_task": {
                "notebook_path": "__DEST_BASE__/01_ingest"
              },
              "existing_cluster_id": "__CLUSTER_ID__",
              "timeout_seconds": 3600
            },
            {
              "task_key": "02_transform",
              "depends_on": [{ "task_key": "01_ingest" }],
              "notebook_task": {
                "notebook_path": "__DEST_BASE__/02_transform"
              },
              "existing_cluster_id": "__CLUSTER_ID__",
              "timeout_seconds": 3600
            },
            {
              "task_key": "03_load",
              "depends_on": [{ "task_key": "02_transform" }],
              "notebook_task": {
                "notebook_path": "__DEST_BASE__/03_load"
              },
              "existing_cluster_id": "__CLUSTER_ID__",
              "timeout_seconds": 3600
            }
          ]
        }
        JSON
        # 2) Sustituir placeholders
        sed -i "s#__DEST_BASE__#${DEST_BASE}#g" job.json
        sed -i "s/__CLUSTER_ID__/${CLUSTER_ID}/g" job.json

        # 3) Validar JSON antes de llamar al API (evita MALFORMED_REQUEST)
        jq . job.json

        # 4) Crear o actualizar el job
        create_response=$(curl -s -X POST \
          -H "Authorization: Bearer $DEST_TOKEN" \
          -H "Content-Type: application/json" \
          --data-binary @job.json \
          "$DEST_HOST/api/2.1/jobs/create")

        # Si ya existe, reset en vez de create
        if echo "$create_response" | jq -e '.error_code=="RESOURCE_ALREADY_EXISTS"' >/dev/null 2>&1; then
          echo "Job ya existe; aplicando reset"
          # Obtener job_id por nombre
          job_id=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.1/jobs/list" | jq -r '.jobs[] | select(.settings.name=="WF_ProyectoFinal") | .job_id')
          curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            --data-binary @job.json \
            "$DEST_HOST/api/2.1/jobs/reset?job_id=${job_id}" >/dev/null
          echo "Reset aplicado al job ${job_id}"
        else
          echo "Workflow creation response: $create_response"
          if echo "$create_response" | jq -e '.job_id' >/dev/null 2>&1; then
            echo "Workflow creado correctamente"
          else
            echo "âŒ Error al crear el workflow"
            echo "Response completo: $create_response"
            exit 1
          fi
        fi
    - name: "Validar workflow (anti-jq exit code 3)"
      env:
          DEST_HOST: ${{ secrets.DATABRICKS_DEST_HOST }}
          DEST_TOKEN: ${{ secrets.DATABRICKS_DEST_TOKEN }}
          # Si en pasos previos hiciste: echo "WORKFLOW_ID=..." >> $GITHUB_ENV
          # entonces aquÃ­ lo recuperamos. Si no existe, mÃ¡s abajo hacemos fallback por nombre.
          WORKFLOW_ID: ${{ env.WORKFLOW_ID }}
        shell: bash
        run: |
          set -euo pipefail

          # Usar variables locales seguras (y permitir fallback a DATABRICKS_* si existieran)
          HOST="${DEST_HOST:-${DATABRICKS_HOST:-}}"
          TOKEN="${DEST_TOKEN:-${DATABRICKS_TOKEN:-}}"

          if [[ -z "$HOST" || -z "$TOKEN" ]]; then
            echo "âŒ Falta DEST_HOST/DEST_TOKEN en este step"; exit 1
          fi

          # Si WORKFLOW_ID no viene en env, lo resolvemos por nombre
          if [[ -z "${WORKFLOW_ID:-}" || "${WORKFLOW_ID}" == "null" ]]; then
            WORKFLOW_ID=$(curl -sS -H "Authorization: Bearer $TOKEN" \
                               "$HOST/api/2.1/jobs/list" \
                         | jq -r '.jobs[] | select(.settings.name=="WF_ProyectoFinal") | .job_id' \
                         | head -n1)
            if [[ -z "$WORKFLOW_ID" ]]; then
              echo "âŒ No se pudo resolver WORKFLOW_ID por nombre"; exit 1
            fi
          fi

          get_json() {
            local url="$1"
            local resp code body
            resp=$(curl -sS -H "Authorization: Bearer $TOKEN" \
                           -H "Content-Type: application/json" \
                           "$url" -w $'\n%{http_code}')
            code=$(echo "$resp" | tail -n1)
            body=$(echo "$resp" | sed '$d')
            if [[ ! "$code" =~ ^2 ]]; then
              echo "HTTP $code in $url"
              echo "BODY (raw):"
              echo "$body"
              exit 1
            fi
            if ! echo "$body" | jq -e . >/dev/null 2>&1; then
              echo "Body is not valid JSON for $url"
              echo "BODY (raw):"
              echo "$body"
              exit 1
            fi
            printf '%s' "$body"
          }

          echo "Validating workflow configuration..."
          WF_JSON=$(get_json "${HOST}/api/2.1/jobs/get?job_id=${WORKFLOW_ID}")

          echo "Workflow found: $(echo "$WF_JSON" | jq -r '.job_id')"
          echo "Name: $(echo "$WF_JSON" | jq -r '.settings.name')"
          echo "Tasks:"
          echo "$WF_JSON" | jq -r '.settings.tasks[] | "- \(.task_key) -> \(.notebook_task.notebook_path)"'
    
    - name: Execute Workflow WF_ProyectoFinal
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        WORKFLOW_NAME="WF_ProyectoFinal"
        
        echo "ğŸš€ Ejecutando workflow: $WORKFLOW_NAME"
        
        # Obtener job_id del workflow
        workflows_list=$(curl -s -X GET \
          -H "Authorization: Bearer $DEST_TOKEN" \
          "$DEST_HOST/api/2.1/jobs/list")
        
        job_id=$(echo "$workflows_list" | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name == $name) | .job_id')
        
        if [ "$job_id" != "" ] && [ "$job_id" != "null" ]; then
          echo "âœ… Workflow WF_ProyectoFinal encontrado con ID: $job_id"
          
          # Ejecutar el workflow
          run_response=$(curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"job_id\": $job_id}" \
            "$DEST_HOST/api/2.1/jobs/run-now")
          
          run_id=$(echo "$run_response" | jq -r '.run_id')
          
          if [ "$run_id" != "" ] && [ "$run_id" != "null" ]; then
            echo "ğŸ¯ Workflow ejecutado exitosamente!"
            echo "Run ID: $run_id"
            echo "WORKFLOW_RUN_ID=$run_id" >> $GITHUB_ENV
            echo "WORKFLOW_JOB_ID=$job_id" >> $GITHUB_ENV
            
            # Mostrar URL del workflow en ejecuciÃ³n
            echo "ğŸ”— URL del workflow: $DEST_HOST/jobs/$job_id/runs/$run_id"
            
          else
            echo "âŒ Error al ejecutar el workflow"
            echo "Response: $run_response"
            exit 1
          fi
        else
          echo "âŒ No se pudo encontrar el workflow para ejecutar"
          exit 1
        fi
    
    - name: Monitor Workflow Execution
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        RUN_ID="${{ env.WORKFLOW_RUN_ID }}"
        JOB_ID="${{ env.WORKFLOW_JOB_ID }}"
        
        echo "ğŸ“Š Monitoreando ejecuciÃ³n del workflow WF_ProyectoFinal..."
        echo "Job ID: $JOB_ID"
        echo "Run ID: $RUN_ID"
        
        # Monitorear por mÃ¡ximo 10 minutos (600 segundos)
        max_wait_time=600
        wait_time=0
        check_interval=30
        
        while [ $wait_time -lt $max_wait_time ]; do
          # Obtener estado actual
          run_status=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.1/jobs/runs/get?run_id=$RUN_ID")
          
          state=$(echo "$run_status" | jq -r '.state.life_cycle_state')
          result_state=$(echo "$run_status" | jq -r '.state.result_state // "RUNNING"')
          
          echo "â±ï¸  Estado actual: $state ($result_state) - Tiempo transcurrido: ${wait_time}s"
          
          # Mostrar progreso de las tareas
          echo "$run_status" | jq -r '.tasks[]? | "  ğŸ“‹ " + .task_key + ": " + .state.life_cycle_state + " (" + (.state.result_state // "RUNNING") + ")"'
          
          case "$state" in
            "TERMINATED")
              if [ "$result_state" = "SUCCESS" ]; then
                echo "ğŸ‰ Â¡Workflow WF_ProyectoFinal completado exitosamente!"
                
                # Mostrar resumen final
                echo ""
                echo "ğŸ“ˆ Resumen de ejecuciÃ³n:"
                echo "$run_status" | jq -r '.tasks[]? | "âœ… " + .task_key + " â†’ " + (.state.result_state // "SUCCESS")'
                
                # Obtener duraciÃ³n
                start_time=$(echo "$run_status" | jq -r '.start_time')
                end_time=$(echo "$run_status" | jq -r '.end_time')
                if [ "$start_time" != "null" ] && [ "$end_time" != "null" ]; then
                  duration=$((($end_time - $start_time) / 1000))
                  echo "â° DuraciÃ³n total: ${duration} segundos"
                fi
                
                exit 0
              else
                echo "âŒ Workflow terminÃ³ con errores: $result_state"
                echo "ğŸ“‹ Detalles de las tareas:"
                echo "$run_status" | jq -r '.tasks[]? | "âŒ " + .task_key + ": " + (.state.result_state // "UNKNOWN")'
                exit 1
              fi
              ;;
            "INTERNAL_ERROR"|"SKIPPED")
              echo "âŒ Workflow fallÃ³ con estado: $state"
              exit 1
              ;;
            *)
              # Estados: PENDING, RUNNING, TERMINATING
              echo "â³ Workflow WF_ProyectoFinal aÃºn ejecutÃ¡ndose..."
              ;;
          esac
          
          sleep $check_interval
          wait_time=$((wait_time + check_interval))
        done
        
        echo "âš ï¸  Timeout: El workflow aÃºn se estÃ¡ ejecutando despuÃ©s de $max_wait_time segundos"
        echo "ğŸ”— Verifica el estado en: $DEST_HOST/jobs/$JOB_ID/runs/$RUN_ID"
        echo "â„¹ï¸  El workflow seguirÃ¡ ejecutÃ¡ndose en Databricks"
        exit 0
    
    - name: Clean up
      run: |
        rm -rf notebooks_to_deploy
        rm -f workflow_config.json
    
    - name: Done
      run: |
        echo "ğŸ‰ Â¡Despliegue y ejecuciÃ³n completados exitosamente!"
        echo ""
        echo "ğŸ“Š Resumen:"
        echo "âœ… Notebooks desplegados: drop_medallion, 00_prep_env, 01_ingest, 02_transform, C"
        echo "ğŸš€ Workflow creado: WF_ProyectoFinal"
        echo "âœ… Tareas configuradas:"
        echo "   - ğŸ§¹ Tarea1_notebook1 (drop_medallion)"
        echo "   - ğŸ§° Tarea2_notebook2 (00_prep_env)"
        echo "   - ğŸ›¢ï¸â¬‡ï¸ Tarea3_notebook2 (01_ingest)"
        echo "   - ğŸ”„ Tarea4_notebook2 (02_transform)"
        echo "   - ğŸ›¢ï¸â¬†ï¸Tarea5_notebook2 (03_load)"
        echo "âœ… Cluster existente: cluster_SD configurado"
        echo "ğŸ§­ Workflow ejecutado automÃ¡ticamente"
        echo ""
        echo "ğŸ”— Accede a tu workspace de Databricks para ver los resultados detallados"