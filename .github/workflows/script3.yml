name: Deploy Databricks Workflow (WF_ProyectoFinal)

on:
  workflow_dispatch:
    inputs:
      run_now:
        description: "Ejecutar el workflow despuÃ©s del deploy"
        type: boolean
        default: true

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Asegurar jq (rÃ¡pido)
        shell: bash
        run: |
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update -y
            sudo apt-get install -y jq
          fi

      - name: Setup Databricks CLI
        uses: databricks/setup-cli@v0.270.0

      # ---------- EXPORT (ORIGIN) OPCIONAL ----------
      - name: Export multiple notebooks (raw) - ORIGIN (opcional)
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_ORIGIN_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_ORIGIN_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${DATABRICKS_HOST:-}" || -z "${DATABRICKS_TOKEN:-}" ]]; then
            echo "â„¹ï¸ No hay credenciales de ORIGIN: se omite export."
            exit 0
          fi
          ORIGIN_BASE="${{ vars.ORIGIN_WORKSPACE_BASE }}"
          if [[ -z "${ORIGIN_BASE}" || "${ORIGIN_BASE}" == "null" ]]; then
            ORIGIN_BASE="${{ vars.WORKSPACE_BASE }}"
          fi
          if [[ -z "${ORIGIN_BASE}" || "${ORIGIN_BASE}" == "null" ]]; then
            echo "â„¹ï¸ No hay ORIGIN_WORKSPACE_BASE/WORKSPACE_BASE definidos; se omite export."
            exit 0
          fi
          echo "ğŸ“¤ Exportando desde ORIGIN: ${ORIGIN_BASE} â†’ ./exported"
          mkdir -p exported
          databricks workspace export-dir "${ORIGIN_BASE}" ./exported --overwrite

      # ---------- IMPORT (DEST) ----------
      - name: Deploy notebooks to Destination Workspace
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_DEST_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_DEST_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${DATABRICKS_HOST:-}" || -z "${DATABRICKS_TOKEN:-}" ]]; then
            echo "âŒ Faltan credenciales DEST (DATABRICKS_DEST_HOST / DATABRICKS_DEST_TOKEN)"; exit 1
          fi
          WORKSPACE_BASE="${{ vars.WORKSPACE_BASE }}"
          if [[ -z "${WORKSPACE_BASE}" || "${WORKSPACE_BASE}" == "null" ]]; then
            echo "âŒ Falta vars.WORKSPACE_BASE"; exit 1
          fi
          if [[ -d "./exported" ]]; then
            echo "ğŸ“¥ Importando a DEST: ./exported â†’ ${WORKSPACE_BASE}"
            databricks workspace import-dir ./exported "${WORKSPACE_BASE}" --overwrite
          else
            echo "â„¹ï¸ No existe ./exported; se asume que los notebooks ya estÃ¡n en ${WORKSPACE_BASE}."
          fi

      # ---------- CREDENCIALES DEST PARA EL RESTO DEL JOB ----------
      - name: Set DEST credentials for Databricks CLI
        shell: bash
        run: |
          set -euo pipefail
          echo "DATABRICKS_HOST=${{ secrets.DATABRICKS_DEST_HOST }}" >> $GITHUB_ENV
          echo "::add-mask::${{ secrets.DATABRICKS_DEST_TOKEN }}"
          echo "DATABRICKS_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}" >> $GITHUB_ENV
          echo "âœ… CLI apuntado a DEST"

      # ---------- TU STEP: Get existing cluster ID (por nombre) ----------
      - name: Get existing cluster ID
        shell: bash
        run: |
          set -euo pipefail
          CLUSTER_NAME="${{ vars.DEST_CLUSTER_NAME }}"
          if [[ -z "${CLUSTER_NAME}" || "${CLUSTER_NAME}" == "null" ]]; then
            CLUSTER_NAME="cluster_SD"
          fi
          echo "ğŸ” Buscando cluster_id para: ${CLUSTER_NAME}"

          CLUSTER_ID=$(databricks clusters list --output JSON \
            | jq -r --arg n "$CLUSTER_NAME" '.clusters[] | select(.cluster_name==$n) | .cluster_id' \
            | head -n1)

          if [[ -z "${CLUSTER_ID}" ]]; then
            echo "âŒ No se encontrÃ³ el clÃºster con nombre '${CLUSTER_NAME}'. Disponibles:"
            databricks clusters list --output JSON | jq -r '.clusters[].cluster_name' | sed 's/^/- /'
            exit 1
          fi

          echo "DEST_CLUSTER_ID=${CLUSTER_ID}" >> $GITHUB_ENV
          echo "âœ… ${CLUSTER_NAME} â†’ ${CLUSTER_ID}"

      - name: Mostrar tareas del workflow
        shell: bash
        run: |
          echo "ğŸš€ Workflow: Proyecto Final"
          echo "  1) ğŸ§¹  drop_medallion"
          echo "  2) ğŸ§°  00_prep_env"
          echo "  3) ğŸ›¢ï¸â¬‡ï¸ 01_ingest"
          echo "  4) ğŸ”„  02_transform"
          echo "  5) ğŸ›¢ï¸â¬†ï¸ 03_load"

      - name: Render job.json
        shell: bash
        run: |
          set -euo pipefail
          cat > job.json <<'JSON'
          {
            "name": "WF_ProyectoFinal",
            "max_concurrent_runs": 1,
            "tasks": [
              {
                "task_key": "drop_medallion",
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/drop_medallion" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "00_prep_env",
                "depends_on": [{ "task_key": "drop_medallion" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/00_prep_env" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "01_ingest",
                "depends_on": [{ "task_key": "00_prep_env" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/01_ingest.py" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "02_transform",
                "depends_on": [{ "task_key": "01_ingest" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/02_transform.py" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              },
              {
                "task_key": "03_load",
                "depends_on": [{ "task_key": "02_transform" }],
                "notebook_task": { "notebook_path": "__WORKSPACE_BASE__/03_load" },
                "existing_cluster_id": "__CLUSTER_ID__",
                "timeout_seconds": 3600
              }
            ]
          }
          JSON

          sed -i "s#__WORKSPACE_BASE__#${{ vars.WORKSPACE_BASE }}#g" job.json
          sed -i "s/__CLUSTER_ID__/${DEST_CLUSTER_ID}/g" job.json
          jq . job.json

      - name: Upsert del job (create o reset)
        shell: bash
        run: |
          set -euo pipefail
          JOB_ID=$(databricks jobs list --output JSON | jq -r '.jobs[] | select(.settings.name=="WF_ProyectoFinal") | .job_id' || true)
          if [[ -n "${JOB_ID}" ]]; then
            echo "Job existente: ${JOB_ID} â†’ reset con nueva configuraciÃ³n"
            databricks jobs reset --job-id "${JOB_ID}" --json-file job.json
          else
            echo "Creando nuevo job WF_ProyectoFinal"
            CREATE_OUT=$(databricks jobs create --json-file job.json)
            JOB_ID=$(echo "$CREATE_OUT" | jq -r '.job_id')
            if [[ -z "${JOB_ID}" || "${JOB_ID}" == "null" ]]; then
              echo "âŒ No se obtuvo job_id al crear el job"; echo "$CREATE_OUT"; exit 1
            fi
          fi
          echo "JOB_ID=${JOB_ID}" >> $GITHUB_ENV
          echo "âœ… Job listo con ID: ${JOB_ID}"

      # --------- ValidaciÃ³n robusta: evita exit code 3 de jq ----------
      - name: Validar workflow y evitar exit code 3 de jq
        shell: bash
        run: |
          set -Eeuo pipefail

          call_api () {
            local method="$1"
            local url="$2"
            local body="${3:-}"

            if [[ -n "$body" ]]; then
              resp=$(curl -sS -X "$method" "$url" \
                -H "Authorization: Bearer $DATABRICKS_TOKEN" \
                -H "Content-Type: application/json" \
                -d "$body" -w '\n%{http_code}')
            else
              resp=$(curl -sS -X "$method" "$url" \
                -H "Authorization: Bearer $DATABRICKS_TOKEN" \
                -H "Content-Type: application/json" \
                -w '\n%{http_code}')
            fi

            code=$(echo "$resp" | tail -n1)
            body=$(echo "$resp" | sed '$d')

            if [[ ! "$code" =~ ^2 ]]; then
              echo "âŒ HTTP $code en $url"
              echo "â€”â€” Respuesta cruda â€”â€”"
              echo "$body"
              exit 1
            fi

            if ! echo "$body" | jq -e . >/dev/null 2>&1; then
              echo "âŒ La respuesta no es JSON vÃ¡lido para $url"
              echo "â€”â€” Respuesta cruda â€”â€”"
              echo "$body"
              exit 1
            fi

            echo "$body"
          }

          echo "ğŸ” Validando la configuraciÃ³n del workflow creado..."
          WF=$(call_api GET "$DATABRICKS_HOST/api/2.1/jobs/get?job_id=${JOB_ID}")
          echo "âœ… Workflow encontrado con ID: $(echo "$WF" | jq -r '.job_id')"
          echo "ğŸ§© Nombre:  $(echo "$WF" | jq -r '.settings.name')"
          echo "ğŸ“¦ Tareas:"
          echo "$WF" | jq -r '.settings.tasks[] | "- \(.task_key) -> \(.notebook_task.notebook_path)"'

      - name: Ejecutar workflow (opcional) y esperar a que termine
        if: ${{ inputs.run_now }}
        shell: bash
        run: |
          set -euo pipefail
          echo "â© Lanzando ejecuciÃ³n para JOB_ID=${JOB_ID}"
          RUN_OUT=$(databricks jobs run-now --job-id "${JOB_ID}")
          echo "$RUN_OUT" | jq .
          RUN_ID=$(echo "$RUN_OUT" | jq -r '.run_id')
          if [[ -z "${RUN_ID}" || "${RUN_ID}" == "null" ]]; then
            echo "âŒ No se obtuvo run_id al ejecutar el job"; exit 1
          fi
          echo "RUN_ID=${RUN_ID}" >> $GITHUB_ENV

          echo "â³ Esperando a que termine la ejecuciÃ³nâ€¦"
          databricks runs wait --run-id "${RUN_ID}" --timeout 7200
          echo "âœ… EjecuciÃ³n completada."
          echo "ğŸ§­ Workflow ejecutado automÃ¡ticamente"
          echo ""
          echo "ğŸ”— Accede a tu workspace de Databricks para ver los resultados detallados:"
          echo "   ${DATABRICKS_HOST}/#job/${JOB_ID}/run/${RUN_ID}"

      - name: Resumen con iconos
        if: ${{ always() }}
        shell: bash
        run: |
          cat << EOF >> $GITHUB_STEP_SUMMARY
          ## ğŸ§­ Pipeline â€” Proyecto Final
          1. ğŸ§¹ **drop_medallion**
          2. ğŸ§° **00_prep_env**
          3. ğŸ›¢ï¸â¬‡ï¸ **01_ingest**
          4. ğŸ”„ **02_transform**
          5. ğŸ›¢ï¸â¬†ï¸ **03_load**

          ---
          ### ğŸ§­ Workflow ejecutado automÃ¡ticamente
          ğŸ”— Accede a tu workspace de Databricks para ver los resultados detallados:  
          ${DATABRICKS_HOST}/#job/${JOB_ID}/run/${RUN_ID}
          EOF
